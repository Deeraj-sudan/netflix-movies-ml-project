{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Deeraj-sudan/netflix-movies-ml-project/blob/main/Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    -\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    -Unsupervised\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**   - **Deeraj Sudan**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ic1abirMgMN"
      },
      "source": [
        "**Project Summary: Clustering Netflix Movies and TV Shows**\n",
        "\n",
        "The aim of this project is to conduct an in-depth analysis of Netflix's vast collection of movies and TV shows as of 2019 using unsupervised machine learning techniques. This dataset, collected from Flixable, provides a snapshot of Netflix's content offering and its evolution over the years. The project will encompass four primary objectives:\n",
        "\n",
        "**1. Exploratory Data Analysis (EDA):** The project begins with an extensive exploration of the dataset. EDA will involve data cleaning, missing value handling, and statistical analysis to gain a comprehensive understanding of the dataset's characteristics. Visualizations will be employed to reveal trends, patterns, and potential outliers in the data. This phase serves as the foundation for subsequent analyses.\n",
        "\n",
        "**2. Content Analysis by Country:** Netflix operates in numerous countries, and the type of content available often varies by region. This project aims to investigate the diversity of content offerings across different countries. By segmenting and analyzing content by region, it will be possible to identify regional preferences and trends, potentially informing Netflix's content acquisition strategies.\n",
        "\n",
        "**3. Focus on TV vs. Movies:** A key aspect of this project is to evaluate whether Netflix's content strategy has shifted over the years. By comparing the number of movies and TV shows available on the platform in 2010 and 2019, it will be possible to determine if Netflix has been increasingly focusing on TV shows as reported by Flixable. This analysis will provide insights into Netflix's evolving content mix.\n",
        "\n",
        "**4. Clustering Similar Content:** The final phase of the project involves applying unsupervised machine learning techniques, specifically clustering, to group similar movies and TV shows. The goal is to identify patterns and similarities in the content based on text-based features, such as titles, genres, and descriptions. Clustering will enable Netflix to better understand its content library and potentially offer personalized recommendations to users.\n",
        "\n",
        "Furthermore, the project suggests the possibility of enhancing the analysis by integrating external datasets, such as IMDB ratings and Rotten Tomatoes scores. This will provide additional context and insights into the quality and reception of Netflix's content.\n",
        "\n",
        "In conclusion, this project explores Netflix's content landscape with the primary goal of offering valuable insights to both the company and its viewers. It will involve thorough data analysis, regional content segmentation, historical trend analysis, and machine learning-driven content clustering. The results will empower Netflix to make data-driven decisions about its content strategy and potentially enhance user experiences by tailoring recommendations based on content similarities. As the digital entertainment landscape continues to evolve, understanding and leveraging data-driven insights are critical for staying competitive, and this project serves as a compelling example of data science's application in the media and entertainment industry."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/Deeraj-sudan/netflix-movies-ml-project"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nL6W-o6IMncc"
      },
      "source": [
        "**Problem Statement:**\n",
        "\n",
        "The objective of this project is to analyze Netflix's extensive collection of movies and TV shows using a dataset that captures their offerings as of 2019. This analysis encompasses four key aspects:\n",
        "\n",
        "**1. Exploratory Data Analysis (EDA):** The initial challenge is to perform a comprehensive exploration of the dataset. This involves data cleansing, handling missing values, and employing statistical and visual analysis to understand the dataset's characteristics and identify any potential anomalies. The EDA phase aims to provide a solid foundation for the subsequent analyses.\n",
        "\n",
        "**2. Content Diversity Across Countries:** Netflix operates in multiple countries, and the content available often varies by region. This project seeks to investigate the diversity of content offerings in different countries. By segmenting and examining content based on geographical regions, the goal is to discern regional content preferences and trends, which can be instrumental in informing Netflix's content acquisition strategies.\n",
        "\n",
        "**3. Shift in Focus: TV vs. Movies:** Another vital aspect is to evaluate Netflix's content strategy over the years. This involves comparing the count of movies and TV shows available on the platform in 2010 and 2019. The central question is whether Netflix has increasingly prioritized TV shows over movies, as indicated in a report by Flixable. This analysis aims to reveal trends in Netflix's content mix, potentially aiding strategic decision-making.\n",
        "\n",
        "**4. Content Clustering:** The final challenge is to leverage unsupervised machine learning techniques to cluster similar movies and TV shows. The primary objective is to uncover patterns and similarities within the content library based on text-based features like titles, genres, and descriptions. Clustering will provide Netflix with insights into its content landscape and enable the potential for personalized recommendations to enhance user experiences.\n",
        "\n",
        "To address these challenges, the project may consider augmenting the analysis by integrating external datasets, such as IMDB ratings and Rotten Tomatoes scores, to gain insights into content quality and reception. The ultimate goal is to provide Netflix with data-driven insights to refine its content strategy and potentially optimize content recommendations for users, ensuring competitiveness and relevance in the rapidly evolving digital entertainment landscape."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UpwyJXazjVxV"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime as dt\n",
        "\n",
        "# Import Data Visualisation Libraries\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# Import and Ignore warnings for better code readability,\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAlHL0hHju3_"
      },
      "outputs": [],
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "# Read csv file\n",
        "data_raw = pd.read_csv('/content/drive/My Drive/Global Terrorism/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')\n",
        "\n",
        "# Creating a copy of data set\n",
        "# Before doing any data wrangling lets create copy of the dataset because it may change original data\n",
        "data = data_raw.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "data.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "cAa5da2zkcxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awlJxXFskr23"
      },
      "outputs": [],
      "source": [
        "# Dataset Rows & Columns count\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "data.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "data.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ajAdlUHEfEOq"
      },
      "outputs": [],
      "source": [
        "data.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge5oFq9BlhqL"
      },
      "outputs": [],
      "source": [
        "# Visualizing the missing values\n",
        "# null value distribution\n",
        "null_counts = data.isnull().sum()/len(data)\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.xticks(np.arange(len(null_counts)),null_counts.index,rotation='vertical')\n",
        "plt.ylabel('fraction of rows with missing data')\n",
        "plt.bar(np.arange(len(null_counts)),null_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8qEiOculq_U"
      },
      "source": [
        "1. show_id: A unique identifier for each show or movie on Netflix.\n",
        "\n",
        "2. type: Indicates whether the entry is a \"TV Show\" or a \"Movie.\"\n",
        "\n",
        "3. title: The title of the show or movie.\n",
        "\n",
        "4. director: The director(s) of the show or movie. This variable contains missing values (NaN).\n",
        "\n",
        "5. cast: The cast or actors in the show or movie. This variable also contains missing values (NaN).\n",
        "\n",
        "6. country: The country or countries where the show or movie is available. This variable contains some missing values.\n",
        "\n",
        "7. date_added: The date when the show or movie was added to Netflix. Some missing values present.\n",
        "\n",
        "8. release_year: The year the show or movie was originally released.\n",
        "\n",
        "9. rating: The content rating assigned to the show or movie. Some missing values present.\n",
        "\n",
        "10. duration: The duration of the show or movie, typically in terms of seasons (for TV shows) or minutes (for movies).\n",
        "\n",
        "11. listed_in: The categories or genres that the show or movie is classified under.\n",
        "\n",
        "12. description: A brief description or summary of the show or movie."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "data.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "data.describe(include='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOJGfJFQl-4k"
      },
      "source": [
        "The output of the describe function provides statistical summary information about the numerical columns in your dataset. However, some columns in your dataset are non-numeric (e.g., 'type,' 'title,' 'director,' 'cast,' etc.), so they are not included in this summary.\n",
        "\n",
        "Here are some insights you can derive from the provided statistics:\n",
        "\n",
        "**Release Year:**\n",
        "The earliest release year in the dataset is 1925, indicating the presence of older content.\n",
        "The latest release year is 2021, which is the most recent content.\n",
        "The median release year is around 2017, indicating that there's a mix of older and more recent content on Netflix.\n",
        "\n",
        "**Rating:**\n",
        "The most frequent content rating is 'TV-MA,' which suggests a preference for mature content.\n",
        "There are 14 unique content ratings in the dataset, indicating diversity in content targeting various age groups.\n",
        "\n",
        "**Duration:**\n",
        "The 'duration' column is not numeric, so summary statistics are not provided here.\n",
        "This column likely contains information about the duration of TV shows in terms of seasons and movies in terms of minutes.\n",
        "\n",
        "**Country:**\n",
        "The 'country' column is non-numeric and has 681 unique values, indicating content from various countries.\n",
        "The most frequent country in the dataset is the 'United States' (occurring 2,555 times).\n",
        "\n",
        "**Date Added:**\n",
        "The 'date_added' column is non-numeric and has 1,565 unique values.\n",
        "The most frequent addition date is 'January 1, 2020' (occurring 118 times).\n",
        "\n",
        "Please note that for non-numeric columns like 'type,' 'title,' 'director,' 'cast,' etc., I need to perform specific analyses (e.g., counts, unique values) to gain further insights. Additionally, handling and exploring missing data in columns like 'director,' 'cast,' and 'country' may be necessary to ensure a comprehensive analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "data.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwloyfvFEPEz"
      },
      "source": [
        "#### Handling Null Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zvm3AVUIAUea"
      },
      "source": [
        "* We found that, there are 3631 null values in the dataset, 2389 null values in director column, 718 null values in cast column ,507 null values in country column ,10 in date_added and 7 in rating.\n",
        "\n",
        "So we are going to handle these null values in following steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fFAyKNfRZLZ"
      },
      "outputs": [],
      "source": [
        "for i in data.columns:\n",
        "  null_rate = data[i].isnull().sum()/len(data)*100\n",
        "  if null_rate > 0 :\n",
        "        print( \"{}'s null rate: {}%\".format(i, round(null_rate, 3)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZgRst2WKps3"
      },
      "outputs": [],
      "source": [
        "data[data['country'].isnull()].head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "outputs": [],
      "source": [
        "# Filling NaN or Missing values in 'cast' column with 'No cast'.\n",
        "data['cast'].fillna(value='No cast',inplace=True)\n",
        "\n",
        "data['country'].fillna('Country Unavailable', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIlTzZqaMFjb"
      },
      "outputs": [],
      "source": [
        "# Analyzing 'rating' columns NaN values in which by prospecting through 'title' and 'cast' we are getting most of the USA(mode) movies don't have 'rating'.\n",
        "data[data['rating'].isna()].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8elUNsoxMb_D"
      },
      "outputs": [],
      "source": [
        "# Analyzing 'date_added' columns NaN values\n",
        "data[data['date_added'].isna()].head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAKU7YtOBZkK"
      },
      "outputs": [],
      "source": [
        "# Dealing with null values in 'rating' feature of dataset\n",
        "data['rating'].value_counts()\n",
        "\n",
        "# When dealing with missing values in the 'rating' column, one common approach is to impute them with the mode since the 'rating' is a categorical variable.\n",
        "# Calculate the mode of the 'rating' column\n",
        "mode_rating = data['rating'].mode()[0]  # Extract the mode value\n",
        "\n",
        "# Impute missing values in the 'rating' column with the mode\n",
        "data['rating'].fillna(mode_rating, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGnt7RvtFjxl"
      },
      "outputs": [],
      "source": [
        "# value_counts() doesn't providing concrete way deal with null values\n",
        "data['date_added'].value_counts()\n",
        "\n",
        "# I noticed in above NaN values visualization for 'date_added' column all NaN values for 'TV Show' and the 'director' for the same rows also missing.\n",
        "# So it is good idea to drop these rows which will handle 20 missing values of our dataset.\n",
        "data.dropna(subset=['date_added'], inplace=True)\n",
        "\n",
        "# 'director' column has considerable amount of null values, i replacing it with 'Not Mentioned'.\n",
        "data['director'] = data['director'].fillna('Not Mentioned')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysM54s-9NOOc"
      },
      "source": [
        "#### Manipulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW-_lkNbLgYA"
      },
      "outputs": [],
      "source": [
        "# Converting the 'date_added' column to a datetime data type, so I can work with dates more effectively.\n",
        "data['date_added'] = pd.to_datetime(data['date_added'], errors='coerce')\n",
        "\n",
        "# Renaming the 'listed_in' column to 'genres'\n",
        "data.rename(columns = {\"listed_in\":\"genres\"},inplace = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRcdwVnWFMcI"
      },
      "outputs": [],
      "source": [
        "# Assigning the ratings into grouped categories\n",
        "ratings = {\n",
        "    'TV-PG': 'Older Kids',\n",
        "    'TV-MA': 'Adults',\n",
        "    'TV-Y7-FV': 'Older Kids',\n",
        "    'TV-Y7': 'Older Kids',\n",
        "    'TV-14': 'Teens',\n",
        "    'R': 'Adults',\n",
        "    'TV-Y': 'Kids',\n",
        "    'NR': 'Adults',\n",
        "    'PG-13': 'Teens',\n",
        "    'TV-G': 'Kids',\n",
        "    'PG': 'Older Kids',\n",
        "    'G': 'Kids',\n",
        "    'UR': 'Adults',\n",
        "    'NC-17': 'Adults'\n",
        "}\n",
        "data['target_ages'] = data['rating'].replace(ratings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1z9mQUJJdL0"
      },
      "outputs": [],
      "source": [
        "data['year_added'] = pd.DatetimeIndex(data['date_added']).year\n",
        "data['month_added'] = pd.DatetimeIndex(data['date_added']).month\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "N1SkEYWCnuSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail()"
      ],
      "metadata": {
        "id": "xIG9eJKxn2Ln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP-_2Srnn9ye"
      },
      "source": [
        "I have performed several important data manipulations to prepare the dataset for analysis. Here's a summary of the manipulations you've done and the resulting insights:\n",
        "\n",
        "**1.Handling Missing Values:**\n",
        "\n",
        "* I addressed missing values in the 'cast' column by replacing them with the string 'No cast.' This allows you to retain the records while indicating the absence of cast information.\n",
        "* For the 'country' column, you imputed missing values to 'Country Unavailable' to ensure minimal impact on the data distribution.\n",
        "* When dealing with missing values in the 'rating' column, one common approach is to impute them with the mode (most frequent value) since the 'rating' is a categorical variable.\n",
        "* Also handled missing values in the 'date_added' column by dropping those rows.\n",
        "* Also handled missing values in the 'director' column by replacing them with the string 'Not Mentioned.' This communicates the absence of date information for those records.\n",
        "\n",
        "**2.Data Type Conversion:**\n",
        "\n",
        "* I converted the 'date_added' column to the datetime data type, making it suitable for date-based analysis.\n",
        "* Renamed the 'listed_in' column to 'genres' which represent to actual data present in the column.\n",
        "* Added 'Year' and 'Month' column for better visualization.\n",
        "\n",
        "**3.Insights:**\n",
        "\n",
        "* As a result of these data manipulations, you now have a cleaner and more structured dataset with non-null values in the relevant columns.\n",
        "* Now we can perform meaningful date-based analysis on the 'date_added' column.\n",
        "* The 'cast' column contains 'No cast' for missing values, helping you identify records with incomplete cast information.\n",
        "* The 'country' column has been handled by imputing missing values with the COuntry Unavailable, which ensures a more complete dataset.\n",
        "* Our dataset now has consistent data types for better analysis and visualization.\n",
        "\n",
        "These manipulations have prepared our dataset for further analysis, allowing us to gain valuable insights into Netflix's content, regional availability, and trends over time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvehr4auJi-y"
      },
      "outputs": [],
      "source": [
        "data.type.value_counts()\n",
        "\n",
        "# Chart - 1 visualization code\n",
        "# Create a figure with two subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Plot the countplot on the first subplot\n",
        "sns.countplot(x='type', data=data, ax=axes[0])\n",
        "axes[0].set_title('Count of Movies and TV Shows')\n",
        "axes[0].set_xlabel('Type')\n",
        "axes[0].set_ylabel('Count')\n",
        "\n",
        "# Plot the pie chart on the second subplot\n",
        "type_counts = data['type'].value_counts()\n",
        "axes[1].pie(type_counts, labels=type_counts.index, autopct='%1.1f%%', startangle=140)\n",
        "axes[1].set_title('Percentage Distribution of Movies and TV Shows')\n",
        "axes[1].axis('equal')  # Equal aspect ratio ensures that the pie chart is circular.\n",
        "\n",
        "# Adjust spacing between subplots\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSbq0f8poggc"
      },
      "source": [
        "Choice of Chart:\n",
        "\n",
        "I picked the \"Countplot\" chart because it's an effective way to visualize the distribution of categorical data. In this case, it allows us to compare the counts of 'Movie' and 'TV Show' in the 'type' column, addressing the objective of understanding the distribution of content types."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veRtduRUokHj"
      },
      "source": [
        "Insights from the Chart:\n",
        "\n",
        "The chart shows that the number of 'Movie' entries is significantly higher than the number of 'TV Show' entries in the dataset. This indicates that Netflix's content library is dominated by movies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18a7xeH0onR-"
      },
      "source": [
        "Business Impact:\n",
        "\n",
        "The insight gained from this chart can have a positive business impact. It provides valuable information about the composition of Netflix's content, helping the company understand the preferences and distribution of its offerings. This insight can guide content acquisition strategies, advertising, and user recommendations.\n",
        "\n",
        "Negative Growth Insights:\n",
        "\n",
        "While the chart does not directly indicate negative growth, it does reveal an imbalance in the distribution of content types. If Netflix intended to focus on TV shows but found that movies dominate its library, this could be considered a deviation from the intended content mix. However, it's essential to consider user preferences and whether the movie-dominated library aligns with user demand. It may not necessarily lead to negative growth if it reflects customer preferences and keeps subscribers engaged. The negative impact would depend on the alignment with user expectations and business objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksNre35dqUgz"
      },
      "outputs": [],
      "source": [
        "# Group the data by 'country' and 'type', and count the occurrences\n",
        "content_count = data.groupby(['country', 'type']).size().unstack().fillna(0)\n",
        "\n",
        "# Sum the counts for 'Movie' and 'TV Show' to get the total content count\n",
        "content_count['Total'] = content_count['Movie'] + content_count['TV Show']\n",
        "\n",
        "# Sort the data by the total content count in descending order and select the top 20 countries\n",
        "top_20_countries = content_count.sort_values(by='Total', ascending=False).head(20)\n",
        "\n",
        "# Display the top 20 countries with their content counts in a table\n",
        "top_20_countries.reset_index(inplace=True)\n",
        "top_20_countries.columns = ['Country', 'Movie', 'TV Show', 'Total']\n",
        "print(top_20_countries)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r0xfwFnAo7KT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JsAdNwYeucNS"
      },
      "outputs": [],
      "source": [
        "# Chart - 2 visualization code\n",
        "# Data for the table\n",
        "countries = top_20_countries['Country']\n",
        "total = top_20_countries['Total']\n",
        "movies = top_20_countries['Movie']\n",
        "tv_shows = top_20_countries['TV Show']\n",
        "\n",
        "# Set the width of the bars\n",
        "bar_width = 0.2\n",
        "\n",
        "# Create an array representing the position of each country on the x-axis\n",
        "x = range(len(countries))\n",
        "\n",
        "# Create the joint bar graph\n",
        "plt.figure(figsize=(14, 7))\n",
        "plt.bar(x, total, width=bar_width, label='Total', align='center')\n",
        "plt.bar([i + bar_width for i in x], movies, width=bar_width, label='Movies', align='center')\n",
        "plt.bar([i + 2 * bar_width for i in x], tv_shows, width=bar_width, label='TV Shows', align='center')\n",
        "\n",
        "# Set the x-axis labels to be the country names\n",
        "plt.xticks([i + bar_width for i in x], countries, rotation=45, fontsize=10)\n",
        "\n",
        "# Add labels and a legend\n",
        "plt.xlabel('Country')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Content Distribution by Type in Top 20 Countries')\n",
        "plt.legend()\n",
        "\n",
        "# Show the graph\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl14uY1u8uuq"
      },
      "source": [
        "Choice of Chart:\n",
        "\n",
        "I picked the \"Bar Chart\" to display a table of the top 20 countries with the most content. While a bar chart is commonly used to visualize data, it's an effective way to present tabular data concisely."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKlgUUjh8yt3"
      },
      "source": [
        "Insights from the Chart:\n",
        "\n",
        "The chart, which is essentially a table, provides insights into the distribution of content (Movies and TV Shows) among the top 20 countries.\n",
        "\n",
        "The data reveals that the United States has the highest number of both movies and TV shows, making it the leading content provider. India and the United Kingdom follow, with India having a significant number of movies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvSU31SK84Fi"
      },
      "source": [
        "Business Impact:\n",
        "\n",
        "The insights gained from this table can have a positive business impact. It helps Netflix understand which countries contribute the most content to their library and can influence content acquisition and localization strategies.\n",
        "\n",
        "Negative Growth Insights:\n",
        "\n",
        "* While the table doesn't directly indicate negative growth, it can suggest opportunities for improvement. If Netflix intended to diversify its content from various countries and found that only a handful are dominating, it might indicate a deviation from the intended content diversity.\n",
        "* The negative impact would depend on the extent to which this deviation aligns with user preferences and regional goals. For example, if Netflix wanted to expand its library from a more extensive range of countries and found that only a few are contributing significantly, it may need to adjust its content acquisition strategies. The potential negative impact is tied to alignment with business goals."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK4i-zOK88Dr"
      },
      "outputs": [],
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(14,6))\n",
        "plt.title('Type Counts wrt Ratings')\n",
        "sns.countplot(x=data['rating'],hue=data['type'],data=data,order=data['rating'].value_counts().index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rW6EY2Zl9DvA"
      },
      "source": [
        "I chose above countplot for visualizing 'rating' count of Movies and TV Show and arranged all 'rating' in descending order."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d45QYR8O9HQe"
      },
      "source": [
        "The countplot shows that almost every category have more movies except TV-Y and TV-Y7 because small kids prefers TV Show more than Movies. R(Adult) category have very less TV Show. Rating count dominated by TV-MA and TV-14 for both 'type'."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWsTAp8B9MWA"
      },
      "source": [
        "Most rating categories dominated by Movies, so we have to focus on audience preference with resprct to age groups. e.g. Adults and Teenager watches both TV Show and Movies but Small Kids watches more TV Shows. So by considering viewership preference we have to content library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8VBlR3nJQrv"
      },
      "outputs": [],
      "source": [
        "# Chart - 4 visualization code\n",
        "# Create a bar plot for content ratings\n",
        "rating_counts = data['rating'].value_counts()\n",
        "rating_counts = rating_counts.sort_index()  # Sort ratings in alphabetical order\n",
        "\n",
        "# Define a list of 14 different random colors\n",
        "colors = ['royalblue', 'forestgreen', 'mediumorchid', 'goldenrod', 'tomato']\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(rating_counts.index, rating_counts.values, color=colors)\n",
        "\n",
        "# Add labels and title\n",
        "plt.xlabel('Content Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Distribution of Content Ratings')\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n",
        "\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# rating column distribution by plotly\n",
        "fig_donut = go.Figure(go.Pie(\n",
        "    labels=data['rating'].value_counts().index,\n",
        "    values=data['rating'].value_counts(),\n",
        "    hole=0.7,\n",
        "    marker=dict(colors=['#b20710', '#221f1f']),\n",
        "    textinfo='percent+label',\n",
        "    rotation=90,\n",
        "))\n",
        "\n",
        "fig_donut.update_traces(\n",
        "    hovertemplate=None,\n",
        "    textposition='outside',\n",
        ")\n",
        "\n",
        "fig_donut.update_layout(\n",
        "    height=800,\n",
        "    width=800,\n",
        "    title='MOST OF PROGRAMME ON NETFLIX IS TV-14 & TV-MA RATED',\n",
        "    margin=dict(t=80, b=10, l=0, r=0),\n",
        "    showlegend=False,\n",
        "    plot_bgcolor='#333',\n",
        "    paper_bgcolor='#333',\n",
        "    title_font=dict(size=25, color='#8a8d93', family=\"Lato, sans-serif\"),\n",
        "    font=dict(size=17, color='#8a8d93'),\n",
        "    hoverlabel=dict(bgcolor=\"#444\", font_size=13, font_family=\"Lato, sans-serif\"),\n",
        ")\n",
        "\n",
        "fig_donut.show(renderer='colab')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpWmtXX79aZk"
      },
      "source": [
        "The selection of above bar garph and dounut chart attracted us because those are beautifully visualizing RATING Distribution and Percentage of Rating in Netflix content Library.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teMVzjap9eNX"
      },
      "source": [
        "The bar graph shows the rating distribution arranged in alphabetical order where we can see that Adult and Older Kids content dominated over Teenager and Small kids.\n",
        "\n",
        "2nd plotly donut plot gives interactive percentage content distribution information for 'rating'. The donut plot shows Netflix's content might be favouring to mass audience choice."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhVfKN2V9h6r"
      },
      "source": [
        "The insight from above plots shows that our content listing should be preferred by considering audience viewership. We also focus on Users demography for anticipating viewership where any change in preference might predicted before audience get shifted to competitors."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4u0lvCDWJ1d3"
      },
      "outputs": [],
      "source": [
        "# Chart - 5 visualization code\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.title('Type Counts wrt Ratings')\n",
        "sns.countplot(x=data['target_ages'],hue=data['type'],data=data,order=data['target_ages'].value_counts().index)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynQTDqxh9ttI"
      },
      "source": [
        "I preferred above graph to visualize content count of Movies and TV Show with respect to 'target_ages'."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQKYifn_9xI_"
      },
      "source": [
        "This is beautifully carved barplot shows descending trend of both Movies and TV Show counts. The descending trend also shows content count for age in both categories also follows descending trend.\n",
        "\n",
        "Means business perspective our preference should be more for Movies over TV Show but we should also focus on age related content listing."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4Qe5NQN91FY"
      },
      "source": [
        "For the positive business impact Netflix should focus on Adult's Content most then Teens, Older Kids and lowest on Kids.\n",
        "\n",
        "Focus also not deviate from Movies over TV Show but Adults and Teens prefer TV Show as well.\n",
        "\n",
        "For Kids we don't have to focus much on Movies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lRTS4QiW_s7O"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code (Modified for 'country' vs 'rating')\n",
        "# Analysing Top 10 countries with most content by rating\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.countplot(x=data['country'], order=data['country'].value_counts().index[0:10], hue=data['target_ages'])\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 10 countries by ratings with target_ages count', fontsize=15, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Krv4ER7iK4w_"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code (Modified for 'country' vs 'rating')\n",
        "# Analysing Top 10 countries with most content by rating\n",
        "plt.figure(figsize=(18,6))\n",
        "sns.countplot(x=data['country'], order=data['country'].value_counts().index[0:10], hue=data['rating'])\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 10 countries by ratings with rating count', fontsize=15, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeKQ14wT-GUv"
      },
      "source": [
        "I picked up above two bar charts to emphasis on project objective to focus on content diversity across countries."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piuHqWjQ-JHg"
      },
      "source": [
        "Broadly i can say that almost every top 10 content contributing team has most content for Adult/MA category but Top 2 country India has given more importance to Teens category. Almost negligible focus to Kids content.\n",
        "\n",
        "Adult and Teens domination shows that Netflix content addition mainly focuses on broad viewership audience."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsxwWBQf-OjF"
      },
      "source": [
        "For positive business point of view company focuses on paying customer first and then their dependents. Here it applies to Adult and Teens content over other two categories.\n",
        "\n",
        "Country like India focuses on Teens because of culture and family structure, indians prefer less adult content.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaPruzJELNAr"
      },
      "outputs": [],
      "source": [
        "data['genres'].value_counts().head(35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xydZ5dmnLODA"
      },
      "outputs": [],
      "source": [
        "# Chart - 7 Visualization code\n",
        "#Analysing Top 10 genre of the movies\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Top 20 Genre of Movies and Shows',fontweight=\"bold\")\n",
        "sns.countplot(y=data['genres'],data=data,order=data['genres'].value_counts().index[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dNA39L6-qvQ"
      },
      "source": [
        "The reason behind choosing above plot to showcase Top 20 Genres count in the content collection."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivykc8p9-uqo"
      },
      "source": [
        "The content Genres dominated by Documentaries followed Stand Up Comedy and Mainstream Movies. Then the place for kids TV it may be TV Show because in Kids category Movies don't have much content. Then Family movies, Dramas and Family movies with comedy come in."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SBj9erX-yVw"
      },
      "source": [
        "For making positive business impact we have to focus on the audience demand by analyzing viwership for particular content category and then focus should be make availbale those on demand content in that geography and demography not forgetting diversity in the content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCePrp2fMAJj"
      },
      "outputs": [],
      "source": [
        "# Chart - 8 visualization code\n",
        "#Analysing Top 10 genre of the movies\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.title('Top 20 Genre of Movies/Shows',fontweight=\"bold\")\n",
        "sns.countplot(y=data['genres'], data=data, hue= 'type', order=data['genres'].value_counts().index[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vlbrhLe-9gI"
      },
      "source": [
        "Visualization of Top 20 Genres count for TV Show and Movie Category in Netflix's Content Library."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY_BrR_Q_BeX"
      },
      "source": [
        "The Top 20 Genres graph shows that it is dominated by Movies because it has only three TV Show by which Kids TV has Lion's share.\n",
        "\n",
        "Despite GenZ and Scientific Kids there is no place for Scifi movies or Shows.\n",
        "\n",
        "Nowadays action and adventure has huge fan following but it is at bottom."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jn9GVecY_GEw"
      },
      "source": [
        "For making positive business impact company need to focus dominating category for new listing as well as also consider parity among different categories and age groups. Try to keep their share according to audience base to avoid customer churning."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QZ_zYpFK3J3z"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dr4xeP9l_Nxf"
      },
      "outputs": [],
      "source": [
        "# Chart - 9 visualization code\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all titles into a single string\n",
        "titles_text = \" \".join(data['genres'])\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(titles_text)\n",
        "\n",
        "# Display the WordCloud\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud for Genres', fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdxtQFLB_VSH"
      },
      "source": [
        "I chose this graph because to show the frequency of specific genre with the help of wordcloud in which bigger size means more frquency."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rugjWBAu_ZJ3"
      },
      "source": [
        "It is evident from the above wordcloud that specific genre has more presence in the content library beacuse size in wordcloud has direct relation with count.\n",
        "\n",
        "TV Shows has highest count followed by International Movies then Dramas International and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQd4S371_c5f"
      },
      "source": [
        "To easily denote specific genre count wordcloud is good way. But by taking action and making positive business impact is final goal. By taking in consideration frequncy and financial reward by specific genre category for the company need to be honoured."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dh0OLJaCMSMt"
      },
      "outputs": [],
      "source": [
        "# Chart - 6 visualization code\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.set(style=\"darkgrid\")\n",
        "ax = sns.countplot(y=\"release_year\", data=data, palette=\"Set2\", hue = 'type', order=data['release_year'].value_counts().index[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX87tGll_nzA"
      },
      "source": [
        "I used above graph to showcase Top 20 release years count bifurcating in TV Show and Movie."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53nVoos6_ryX"
      },
      "source": [
        "The above graph gives insight that cumulatively 2018 was year which give highest release followed by 2017 and 2019 but 2020 shows fall may be because of Covid 19."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63FIniTZ_vww"
      },
      "source": [
        "The number of movies in release year over the years shows content abundence but for making profit from viewership that content must be of good quality to attact more subscribers."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-Gv6xqyMggS"
      },
      "outputs": [],
      "source": [
        "# Chart - 11 visualization code (for 'country' vs 'year_added')\n",
        "# Analysing Top 10 countries with the most content by release year\n",
        "plt.figure(figsize=(18, 8))\n",
        "sns.countplot(x=data['country'], order=data['country'].value_counts().index[0:10], hue=data['year_added'])\n",
        "plt.xticks(rotation=50)\n",
        "plt.title('Top 10 countries with the most content by year_added', fontsize=15, fontweight='bold')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys7X3zx8_97H"
      },
      "source": [
        "Above graph is giving us idea about count of content getting added in specific year with respect to country."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5LgxvwKABvh"
      },
      "source": [
        "The above graph can give idea about content getting added in specific year no doubt getting reflected in company's Subscribers, Viewership, Profit and maybe share price."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9q2ArNmyAFiY"
      },
      "source": [
        "So it will be attractive place for investing, advertising because more and diverse content will drive in more subscirbers for accessing diverse and quality content from diverse background from varied demographies and geographies. And this cycle will continue..."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6czcl9TAJ97"
      },
      "outputs": [],
      "source": [
        "# Chart - 12 visualization code\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.set(style=\"darkgrid\")\n",
        "ax = sns.countplot(y=\"date_added\", data=data, palette=\"Set2\", hue = 'type', order=data['date_added'].value_counts().index[0:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAKYMIgrAQI_"
      },
      "source": [
        "The above graph shows Top 20 content addition dates bifurcating added content in two categories MOvies and TV Shows."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrmSDvuQAUv_"
      },
      "source": [
        "Overall in TOP 20 content added wrt date 2018 and 2019 year are dominating for both categories of content.\n",
        "\n",
        "This shows in recent years Netflix is including more and more content to their library to attract more subscribers and study their viewership pattern and more content of their choice and so on."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrZ27qIAAZSQ"
      },
      "source": [
        "The diverse and more content will attract many advertisers as well as premium subscribers to the arena. Hence more profit would be helpful for adding more content over diverse content and geographies to spread subscriber base and attract viewership to watch inter country content and many services."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eP6l1MhUAeoz"
      },
      "outputs": [],
      "source": [
        "# Chart - 13 visualization code\n",
        "# Creating two extra columns\n",
        "Movies = data[data['type'] == 'Movie']\n",
        "TV_shows = data[data['type'] == 'TV Show']\n",
        "\n",
        "Movies_year = Movies['release_year'].value_counts().sort_index(ascending=False)\n",
        "TV_shows_year = TV_shows['release_year'].value_counts().sort_index(ascending=False)\n",
        "\n",
        "# Visualizing the Movies and TV_shows based on the release_year\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.set(font_scale=1.4)\n",
        "sns.lineplot(data=Movies_year, label=\"Movies / year\", color='maroon')\n",
        "sns.lineplot(data=TV_shows_year, label=\"TV Shows / year\", color='blue')\n",
        "plt.xlabel(\"Years\", labelpad=15)\n",
        "plt.ylabel(\"Number\", labelpad=15)\n",
        "plt.title(\"Yearly Production Stats\", y=1.02, fontsize=22)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJMyn05MArTP"
      },
      "source": [
        "I chose a line plot for this visualization because it is well-suited to show trends and changes over a continuous variable, such as years. In this case, we want to analyze the production trends of movies and TV shows over the years. A line plot allows us to observe how the number of movies and TV shows released each year has changed over time. By using two lines for movies and TV shows, we can easily compare their production trends."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb2iSMApAvvv"
      },
      "source": [
        "The line plot shows the yearly production statistics for movies and TV shows. Some key insights from the chart are:\n",
        "* The production of both movies and TV shows has generally increased over the years.\n",
        "* The number of movies produced has been consistently higher than TV shows, with a few exceptions.\n",
        "* There was a noticeable surge in TV show production starting from around 2015.\n",
        "* The year 2020 saw a significant increase in the production of both movies and TV shows, which could be attributed to various factors, including the popularity of streaming platforms."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKE08cBXA4cW"
      },
      "source": [
        "* The gained insights can potentially help create a positive business impact, particularly for streaming platforms and production companies. Understanding the trends in production can guide content creators and distributors in making informed decisions about their content portfolios.\n",
        "* The positive insights include the overall growth in production, which indicates a growing demand for content. This could be an opportunity for businesses to invest in content creation and distribution.\n",
        "* The insight about the surge in TV show production since around 2015 is crucial. Streaming platforms, which are major players in the TV show industry, can use this information to continue producing TV shows that align with consumer preferences.\n",
        "* The significant increase in production in 2020 may be due to the changing entertainment landscape during the COVID-19 pandemic. While it presents an immediate opportunity, it may also result in a more competitive market in the future, which businesses need to be prepared for.\n",
        "* However, there is no direct insight pointing to negative growth. The consistent growth in production, especially with the rise of streaming platforms, is generally positive for the industry. It's essential for businesses to adapt to this evolving landscape to stay competitive and capitalize on the positive trends.\n",
        "\n",
        "\n",
        "The insights gained from this visualization can inform business strategies, content planning, and investment decisions in the entertainment industry."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdUqlShwA8kp"
      },
      "outputs": [],
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# Correlation Heatmap visualization code\n",
        "# Calculate the correlation matrix\n",
        "correlation_matrix = data.corr()\n",
        "\n",
        "# Create a heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
        "plt.title(\"Correlation Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdCNRCCbBK4z"
      },
      "outputs": [],
      "source": [
        "# Pair Plot visualization code\n",
        "# Select relevant numeric columns for the pair plot\n",
        "numeric_columns = ['release_year', 'year_added', 'month_added']\n",
        "\n",
        "# Create a pair plot\n",
        "sns.pairplot(data[numeric_columns], diag_kind='kde')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YatwxXzUBQmh"
      },
      "source": [
        "Pair charts (scatter matrix) are used for analyzing relationships between pairs of variables, helping identify patterns, correlations, and potential outliers in multivariate data, enabling a quick visual exploration of data complexity."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qjGKQkIBURG"
      },
      "source": [
        "There is a significant increase in realease year since 1950 to 2000 so constant increase in content is visible on NETFLIX as well."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhVR1mT4BfwP"
      },
      "source": [
        "To formulate three hypothetical statements based on the dataset, let's assume you have used PCA to reduce the dimensionality of your data. Here are three hypothetical statements:\n",
        "\n",
        "***Hypothetical Statement 1: Content Type and Region***\n",
        "\n",
        "Statement: The distribution of content types (TV shows and movies) varies significantly among different regions where Netflix content is available.\n",
        "\n",
        "***Hypothetical Statement 2: Content Type and Year***\n",
        "\n",
        "Statement: There is a significant change in the distribution of content types (TV shows and movies) over the years, reflecting Netflix's shifting focus.\n",
        "\n",
        "***Hypothetical Statement 3: Duration and Content Type***\n",
        "\n",
        "Statement: The duration (runtime) of content (movies and TV shows) differs significantly between the two types, with one type generally having longer durations.\n",
        "\n",
        "For each statement, I will provide code and statistical testing to obtain a final conclusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI5sc1CRBpb3"
      },
      "source": [
        "### Hypothetical Statement - 1: Content Type Trend: The Distribution of Movie Genres on Netflix Varies Between Countries over the years."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rm-DD8g0Btt2"
      },
      "source": [
        "H0 (Null Hypothesis): Netflix has a consistent focus on both TV shows and movies across all regions.\n",
        "\n",
        "H1 (Alternative Hypothesis): Netflix has been increasingly focusing on TV shows in recent years in most regions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ACd5G1dKBxos"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load your Netflix dataset\n",
        "# Replace 'type' with the actual column name containing content type information\n",
        "# Replace 'release_year' with the actual column name containing release year information\n",
        "\n",
        "# Create separate dataframes for TV shows and movies\n",
        "tv_shows = data[data['type'] == 'TV Show']\n",
        "movies = data[data['type'] == 'Movie']\n",
        "\n",
        "# Calculate the annual count of TV shows and movies\n",
        "tv_show_counts = tv_shows.groupby('year_added').size()\n",
        "movie_counts = movies.groupby('year_added').size()\n",
        "\n",
        "# Perform a t-test to compare the counts\n",
        "t_stat, p_value = ttest_ind(tv_show_counts, movie_counts, equal_var=False)\n",
        "\n",
        "# Set your significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "print(f'T-statistic: {t_stat}')\n",
        "print(f'P-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print('Reject the null hypothesis: The number of TV shows and movies on Netflix has significantly changed over the years.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis: The number of TV shows and movies on Netflix has remained relatively stable over the years.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKRUAtLUB20j"
      },
      "source": [
        "I used the chi-squared test for independence to obtain the p-value."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bbi0vK-B7QY"
      },
      "source": [
        "I chose the chi-squared test for independence because it's an appropriate statistical test for analyzing the relationship between two categorical variables, in this case, \"content type\" (TV shows and movies) and \"region\" (different geographic regions where Netflix content is available). The null hypothesis (H0) assumes that there is no significant association or difference between content type and region. The alternative hypothesis (H1) suggests that there is a significant relationship or difference.\n",
        "\n",
        "The chi-squared test helps determine whether the proportions of TV shows and movies have significantly changed over recent years in different regions. By performing the chi-squared test for each region and examining the p-values, we can decide whether to reject the null hypothesis or not. In this specific case, if the p-value is less than the chosen significance level (alpha), we conclude that Netflix has been increasingly focusing on TV shows in that specific region during recent years."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehEQzbTKB_eP"
      },
      "source": [
        "### Hypothetical Statement - 2: Geographic Variation in Content: Netflix Has Shown a Preference for Producing Original TV Shows Over Movies over geographies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeaLMSoTB_es"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMOuslMB_es"
      },
      "source": [
        "H0 (Null Hypothesis): The type of content available on Netflix is consistent across different countries.\n",
        "\n",
        "H1 (Alternative Hypothesis): There are significant differences in the type of content (movies and TV shows) available on Netflix across different countries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB-lPrBXB_et"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZc6Fr0hB_et"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Load your Netflix dataset\n",
        "# Replace 'type' with the actual column name containing content type information\n",
        "# Replace 'country' with the actual column name containing country information\n",
        "\n",
        "# Create a contingency table of content type and country\n",
        "contingency_table = pd.crosstab(data['type'], data['country'])\n",
        "\n",
        "# Perform a chi-squared test\n",
        "chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
        "\n",
        "# Set your significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "print(f'Chi-squared statistic: {chi2_stat}')\n",
        "print(f'P-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print('Reject the null hypothesis: There are significant differences in content type distribution across countries.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis: The type of content available on Netflix is consistent across different countries.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEg4OPzfB_eu"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TE_CWbTB_eu"
      },
      "source": [
        "I used a chi-squared test to obtain the p-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnB82ZmKB_eu"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_djqUsJqB_ev"
      },
      "source": [
        "I chose the chi-squared test for independence because it's an appropriate statistical test for analyzing the relationship between two categorical variables, in this case, \"content type\" (TV shows and movies) and \"country\" (different countries where Netflix content is available).\n",
        "\n",
        "The null hypothesis (H0) in this test assumes that there is no significant association or difference between content type and country. The alternative hypothesis (H1) suggests that there is a significant relationship or difference. By performing the chi-squared test and examining the p-value, we can determine whether to reject the null hypothesis or not.\n",
        "\n",
        "In this specific case, the very low p-value (close to 0) indicates that there is a significant difference in content type distribution across different countries. Therefore, we reject the null hypothesis and conclude that there are indeed significant differences in the types of content available on Netflix across various countries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqSCuUUuC3Lw"
      },
      "source": [
        "### Hypothetical Statement - 3: Content Preferences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJCxpHL-C3Lx"
      },
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYMxh940C3Ly"
      },
      "source": [
        "H0 (Null Hypothesis): The number of TV shows and movies available on Netflix has remained relatively stable over the years.\n",
        "\n",
        "H1 (Alternative Hypothesis): The number of TV shows available on Netflix has significantly increased over the years, while the number of movies has decreased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLc3Vl4WC3Lz"
      },
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALd5FegSC3L0"
      },
      "outputs": [],
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Load your Netflix dataset\n",
        "# Replace 'type' with the actual column name containing content type information\n",
        "# Replace 'added_year' with the actual column name containing release year information\n",
        "\n",
        "# Create separate dataframes for TV shows and movies\n",
        "tv_shows = data[data['type'] == 'TV Show']\n",
        "movies = data[data['type'] == 'Movie']\n",
        "\n",
        "# Calculate the annual count of TV shows and movies\n",
        "tv_show_counts = tv_shows.groupby('year_added').size()\n",
        "movie_counts = movies.groupby('year_added').size()\n",
        "\n",
        "# Perform a t-test to compare the counts\n",
        "t_stat, p_value = ttest_ind(tv_show_counts, movie_counts, equal_var=False)\n",
        "\n",
        "# Set your significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Print the results\n",
        "print(f'T-statistic: {t_stat}')\n",
        "print(f'P-value: {p_value}')\n",
        "\n",
        "if p_value < alpha:\n",
        "    print('Reject the null hypothesis: The number of TV shows and movies on Netflix has significantly changed over the years.')\n",
        "else:\n",
        "    print('Fail to reject the null hypothesis: The number of TV shows and movies on Netflix has remained relatively stable over the years.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhYYEf3cC3L1"
      },
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjVLEJzXC3L2"
      },
      "source": [
        "I used a two-sample independent t-test to obtain the p-value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqD7A87JC3L2"
      },
      "source": [
        "##### Why did you choose the specific statistical test?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG8Is4wpC3L3"
      },
      "source": [
        "I chose the two-sample independent t-test because it is a suitable statistical test for comparing the means of two independent groups. In this case, the two groups are TV shows and movies, and we are interested in comparing the means of their counts (the number of TV shows and movies) over the years. The t-test helps determine whether the difference in means between these two groups is statistically significant.\n",
        "\n",
        "The null hypothesis (H0) assumes that there is no significant difference in the number of TV shows and movies over the years, while the alternative hypothesis (H1) suggests that there is a significant difference. By conducting the t-test and examining the p-value, we can decide whether to reject the null hypothesis or not. In this specific case, since the p-value is greater than the chosen significance level (alpha), we fail to reject the null hypothesis, indicating that the number of TV shows and movies on Netflix has remained relatively stable over the years."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "data.isnull().any()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbKKqI1jDf3_"
      },
      "outputs": [],
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Select only the numerical columns\n",
        "numerical_columns = data.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "# Create box plots for each numerical column\n",
        "for column in numerical_columns.columns:\n",
        "    plt.figure(figsize=(8, 4))  # Adjust the figure size as needed\n",
        "    plt.boxplot(numerical_columns[column], vert=False)\n",
        "    plt.title(f'Box Plot for {column}')\n",
        "    plt.xlabel(column)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "data.info()"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXrevE8cy2nM"
      },
      "outputs": [],
      "source": [
        "# Encode your categorical columns\n",
        "\n",
        "# Define a function to process the 'country' column\n",
        "def process_country(country):\n",
        "    # Split the country string into tokens\n",
        "    tokens = country.split(', ')\n",
        "\n",
        "    # Check the number of tokens\n",
        "    if len(tokens) == 1:\n",
        "        # Keep the single token as is\n",
        "        return country\n",
        "    else:\n",
        "        # Replace with 'Other' for multiple tokens\n",
        "        return 'Multi Country'\n",
        "\n",
        "# Apply the function to the 'country' column\n",
        "data['country'] = data['country'].apply(process_country)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QA4auetzBd_"
      },
      "outputs": [],
      "source": [
        "# Import the necessary library\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Create a LabelBinarizer\n",
        "lb = LabelBinarizer()\n",
        "\n",
        "# Encode the 'type' column\n",
        "data['type_encoded'] = lb.fit_transform(data['type'])\n",
        "data.drop('type', axis=1, inplace=True)\n",
        "# Now, the 'type_encoded' column contains 1 for 'TV Show' and 0 for 'Movie'\n",
        "\n",
        "#data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDcLZVfGwHA3"
      },
      "outputs": [],
      "source": [
        "# Import necessary the library\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Create a LabelEncoder\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply label encoding to the 'country' column\n",
        "data['country_encoded'] = le.fit_transform(data['country'])\n",
        "data.drop('country', axis=1, inplace=True)\n",
        "\n",
        "#data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCB0WcJqZ8-g"
      },
      "outputs": [],
      "source": [
        "data['seasons'] = data['duration'].str.extract(r'(\\d+) Seasons?')\n",
        "data['minutes'] = data['duration'].str.extract(r'(\\d+) min')\n",
        "data['minutes'] = data['minutes'].fillna(0).astype(int)\n",
        "data['seasons'] = data['seasons'].fillna(0).astype(int)\n",
        "data.drop('duration', axis=1, inplace=True)\n",
        "\n",
        "#data.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTK3nYvDysH2"
      },
      "outputs": [],
      "source": [
        "# Create a LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Encode 'rating' and 'target_ages'\n",
        "data['rating_encoded'] = label_encoder.fit_transform(data['rating'])\n",
        "data['target_ages_encoded'] = label_encoder.fit_transform(data['target_ages'])\n",
        "\n",
        "# Get the mapping for 'rating' and 'target_ages'\n",
        "rating_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "target_ages_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
        "\n",
        "# Print the mapping\n",
        "print(\"Rating Mapping:\")\n",
        "print(rating_mapping)\n",
        "\n",
        "print(\"Target Ages Mapping:\")\n",
        "print(target_ages_mapping)\n",
        "\n",
        "data.drop('rating', axis=1, inplace=True)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K82PgFDEEcEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wilOx4zF8ma_"
      },
      "outputs": [],
      "source": [
        "# Create a new column named \"combined_text\" by concatenating the values from best suitable text columns\n",
        "data['combined_text'] = data['genres'] + ' ' + data['description']\n",
        "\n",
        "# Now, the \"combined_text\" column contains the concatenated values\n",
        "\n",
        "# Define the desired order of column names\n",
        "desired_column_order = [\n",
        "    'show_id', 'title', 'director', 'cast', 'date_added', 'release_year', 'genres', 'description', 'target_ages', 'combined_text',\n",
        "    'type_encoded', 'country_encoded', 'rating_encoded', 'target_ages_encoded', 'seasons', 'minutes', 'year_added', 'month_added'\n",
        "    ]\n",
        "\n",
        "# Create a new DataFrame with the columns in the desired order\n",
        "data = data[desired_column_order]\n",
        "\n",
        "# Now, the columns are reordered as per your desired order in the new DataFrame.\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqKfcp4MEi3n"
      },
      "source": [
        "I have used a combination of categorical encoding techniques in this project to handle different types of categorical data:\n",
        "\n",
        "**LabelBinarizer for 'type' Column:** I used LabelBinarizer to encode the 'type' column because it contains two distinct categories ('Movie' and 'TV Show'). I chose this technique to create a single binary column that efficiently represents this binary classification. In this encoding, 'TV Show' is represented as 1, and 'Movie' as 0.\n",
        "\n",
        "**LabelEncoder for 'country' Column:** For the 'country' column, I employed LabelEncoder to encode the country names into numerical values. Although this column has multiple categories, it doesn't have a natural ordinal relationship, making LabelEncoder an appropriate choice. It assigns unique integers to each country.\n",
        "\n",
        "**LabelEncoder for 'rating' Column:** Similarly, I used LabelEncoder for the 'rating' column, which contains different content ratings (e.g., 'TV-MA', 'TV-14'). I opted for LabelEncoder because ratings are typically non-ordinal and don't follow a specific order. It assigns unique integers to each rating.\n",
        "\n",
        "**LabelEncoder for 'target_ages' Column:** The 'target_ages' column represents age groups (e.g., '7+', '13+'). I used LabelEncoder for this column as well. While these categories may have a meaningful order, they are often treated as non-ordinal categories in practice. LabelEncoder assigns unique integers to each age group.\n",
        "\n",
        "The choice of encoding technique for each column was made based on the nature of the data and the specific requirements of the analysis and machine learning models. This combination of techniques allowed me to effectively encode and transform the categorical data for further analysis and modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Wnief3CcelZ"
      },
      "outputs": [],
      "source": [
        "!pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVWquWCXEvGW"
      },
      "outputs": [],
      "source": [
        "# Expand Contraction\n",
        "# Define a function to expand common contractions\n",
        "def expand_contractions(text):\n",
        "    contractions_dict = {\n",
        "        \"ain't\": \"is not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"can't've\": \"cannot have\",\n",
        "        \"'cause\": \"because\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"he'd\": \"he would\",\n",
        "        \"he'd've\": \"he would have\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"he'll've\": \"he will have\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"how'd'y\": \"how do you\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"I'd\": \"I would\",\n",
        "        \"I'd've\": \"I would have\",\n",
        "        \"I'll\": \"I will\",\n",
        "        \"I'll've\": \"I will have\",\n",
        "        \"I'm\": \"I am\",\n",
        "        \"I've\": \"I have\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"it'd\": \"it would\",\n",
        "        \"it'd've\": \"it would have\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"it'll've\": \"it will have\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"mayn't\": \"may not\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"mightn't\": \"might not\",\n",
        "        \"mightn't've\": \"might not have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"mustn't've\": \"must not have\",\n",
        "        \"needn't\": \"need not\",\n",
        "        \"needn't've\": \"need not have\",\n",
        "        \"o'clock\": \"of the clock\",\n",
        "        \"oughtn't\": \"ought not\",\n",
        "        \"oughtn't've\": \"ought not have\",\n",
        "        \"shan't\": \"shall not\",\n",
        "        \"sha'n't\": \"shall not\",\n",
        "        \"shan't've\": \"shall not have\",\n",
        "        \"she'd\": \"she would\",\n",
        "        \"she'd've\": \"she would have\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"she'll've\": \"she will have\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"shouldn't've\": \"should not have\",\n",
        "        \"so've\": \"so have\",\n",
        "        \"so's\": \"so is\",\n",
        "        \"that'd\": \"that would\",\n",
        "        \"that'd've\": \"that would have\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"there'd\": \"there would\",\n",
        "        \"there'd've\": \"there would have\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"they'd\": \"they would\",\n",
        "        \"they'd've\": \"they would have\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"they'll've\": \"they will have\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"to've\": \"to have\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"we'd\": \"we would\",\n",
        "        \"we'd've\": \"we would have\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"we'll've\": \"we will have\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"what'll've\": \"what will have\",\n",
        "        \"what're\": \"what are\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"when've\": \"when have\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"where've\": \"where have\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"who'll've\": \"who will have\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"who've\": \"who have\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"why've\": \"why have\",\n",
        "        \"will've\": \"will have\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"won't've\": \"will not have\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"wouldn't've\": \"would not have\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"y'all'd\": \"you all would\",\n",
        "        \"y'all'd've\": \"you all would have\",\n",
        "        \"y'all're\": \"you all are\",\n",
        "        \"y'all've\": \"you all have\",\n",
        "        \"you'd\": \"you would\",\n",
        "        \"you'd've\": \"you would have\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"you'll've\": \"you will have\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"you've\": \"you have\"\n",
        "    }\n",
        "\n",
        "    # Iterate over contractions dictionary and replace them in the text\n",
        "    for key, value in contractions_dict.items():\n",
        "        text = text.replace(key, value)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply the function to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(expand_contractions)\n",
        "\n",
        "# The contractions in the DataFrame have been expanded"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvlzG-PdE8hf"
      },
      "outputs": [],
      "source": [
        "# Lower Casing\n",
        "# Apply lower casing to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].str.lower()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qP06A02kFBd_"
      },
      "outputs": [],
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "# Define a function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Apply the function to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(remove_punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob_gtW-LFHfO"
      },
      "outputs": [],
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "# Define a function to remove URLs\n",
        "def remove_urls(text):\n",
        "    return re.sub(r'http\\S+|www.\\S+', '', text)\n",
        "\n",
        "# Define a function to remove words and digits containing digits\n",
        "def remove_words_with_digits(text):\n",
        "    return ' '.join(word for word in text.split() if not any(char.isdigit() for char in word))\n",
        "\n",
        "# Apply the functions to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(remove_urls)\n",
        "data['combined_text'] = data['combined_text'].apply(remove_words_with_digits)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pm00sO9PFO9B"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfbivIfjFTl3"
      },
      "outputs": [],
      "source": [
        "# Remove Stopwords & White spaces\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Define a function to remove stopwords and white spaces\n",
        "def remove_stopwords_and_whitespace(text):\n",
        "    # Tokenize the text\n",
        "    words = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    # Join the remaining words and remove extra white spaces\n",
        "    return ' '.join(words)\n",
        "\n",
        "# Apply the function to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(remove_stopwords_and_whitespace)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEF714tKFZq-"
      },
      "outputs": [],
      "source": [
        "# Rephrase Text\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to rephrase text\n",
        "def rephrase_text(text):\n",
        "    # Tokenize the text into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Rephrase each word using its synonyms\n",
        "    rephrased_words = []\n",
        "    for word in words:\n",
        "        synsets = wordnet.synsets(word)\n",
        "        if synsets:\n",
        "            # Use the first synonym if available\n",
        "            synonym = synsets[0].lemmas()[0].name()\n",
        "            rephrased_words.append(synonym)\n",
        "        else:\n",
        "            rephrased_words.append(word)\n",
        "\n",
        "    # Join the rephrased words to form the rephrased text\n",
        "    return ' '.join(rephrased_words)\n",
        "\n",
        "# Apply the rephrasing function to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(rephrase_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFmzXibDFkgH"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Define a function to tokenize text\n",
        "def tokenize_text(text):\n",
        "    # Use NLTK's word_tokenize to tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Apply the tokenization function to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(tokenize_text)\n",
        "\n",
        "#data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGa9hL9UFqGX"
      },
      "outputs": [],
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK data (if not already downloaded)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define a function to lemmatize text using NLTK\n",
        "def lemmatize_text(text):\n",
        "    words = text  # If your text is already tokenized\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    return lemmatized_words\n",
        "\n",
        "# Apply the function to lemmatize text to the \"combined_text\" column\n",
        "data['combined_text'] = data['combined_text'].apply(lemmatize_text)\n",
        "\n",
        "#data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w36yqI6Fv3u"
      },
      "source": [
        "In this project, I employed the **text normalization technique of Lemmatization**. Lemmatization is the process of reducing words to their base or root form, known as a lemma. I chose Lemmatization for the following reasons:\n",
        "\n",
        "**Semantic Accuracy:** Lemmatization ensures that words are transformed into their most basic, dictionary form, which is a lemma. This helps maintain the semantic accuracy of the text. For example, it reduces different inflected forms of a word to a common base, such as 'running' to 'run' or 'better' to 'good.'\n",
        "\n",
        "**Improved Information Retrieval:** By reducing words to their lemmas, Lemmatization can help improve information retrieval and text understanding. It ensures that different forms of a word are recognized as the same word, leading to better search and analysis results.\n",
        "\n",
        "**Reduced Dimensionality:** Lemmatization can contribute to reduced dimensionality in the text data. By mapping similar words to a common lemma, the vocabulary size decreases, making the data more manageable for analysis.\n",
        "\n",
        "**Interpretability:** Lemmatized text is more interpretable because it represents the underlying meaning of words in the document. This can be especially valuable for tasks that require model interpretability or understanding the significance of certain terms in the analysis.\n",
        "\n",
        "**Compatibility with Text Vectorization:** Lemmatization is compatible with text vectorization techniques like TF-IDF or Word Embeddings. It ensures that the vectors represent the meaningful aspects of the text data, not just variations of the same word.\n",
        "\n",
        "**Improved Model Performance:** In some NLP tasks, Lemmatization can lead to improved model performance. It helps models focus on the essence of words, rather than being distracted by numerous inflected forms.\n",
        "\n",
        "In summary, I chose Lemmatization as the text normalization technique for this project to enhance the quality and interpretability of the text data. It is a valuable method for capturing the semantic meaning of words and ensuring that the text is in a more analytically useful and structured form for subsequent NLP analysis.\n",
        "\n",
        "I used Lemmatization for text Normalization, though Stemming is also good because of less computational nature but it always does not produce meaningful words. That's why I decided go with Lemmatization although it is quite slow but produces meaningful base words."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z_n6kfIF1qv"
      },
      "outputs": [],
      "source": [
        "# POS Taging\n",
        "# import nltk\n",
        "# from nltk import pos_tag\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# # Download NLTK data (if not already downloaded)\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# # Define a function to perform POS tagging on text\n",
        "# def pos_tag_text(text):\n",
        "#     # If text is already a string, tokenize and tag it\n",
        "#     if isinstance(text, str):\n",
        "#         words = word_tokenize(text)\n",
        "#         pos_tags = pos_tag(words)\n",
        "#         return pos_tags\n",
        "#     else:\n",
        "#         # Handle non-string values, e.g., NaN\n",
        "#         return []\n",
        "\n",
        "# # Apply the function to the \"combined_text\" column\n",
        "# data['combined_text'] = data['combined_text'].apply(pos_tag_text)\n",
        "\n",
        "# data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjZH1WPaF6bQ"
      },
      "outputs": [],
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert the list of tokens in the \"combined_text\" column back to strings\n",
        "data['combined_text'] = data['combined_text'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the number of features as needed\n",
        "\n",
        "# Fit and transform the vectorizer on the combined text\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['combined_text'])\n",
        "\n",
        "# Create a DataFrame from the TF-IDF matrix\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Concatenate the TF-IDF DataFrame with the original DataFrame\n",
        "new_data = pd.concat([data, tfidf_df], axis=1)\n",
        "\n",
        "# Now, the DataFrame contains TF-IDF vectors for the \"combined_text\" column\n",
        "new_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke7zYLg5j_8f"
      },
      "outputs": [],
      "source": [
        "new_data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIFp5cL7kNj8"
      },
      "outputs": [],
      "source": [
        "new_data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS3eqoPCki_Q"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'new_data' is your dataset with a 'combined_text' column\n",
        "combined_text = \" \".join(new_data['combined_text'])  # Combine all text from the 'combined_text' column\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
        "\n",
        "# Display the word cloud using matplotlib\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3q19ms6G4zP"
      },
      "source": [
        "I have used the **TfidfVectorizer** (Term Frequency-Inverse Document Frequency Vectorizer) for text vectorization in this project. TfidfVectorizer is a popular technique for converting textual data into numerical vectors, and I chose it for the following reasons:\n",
        "\n",
        "**Term Frequency-Inverse Document Frequency (TF-IDF):** TfidfVectorizer leverages the TF-IDF scheme, which is a valuable statistical measure in NLP. It takes into account the frequency of a term in a document (Term Frequency) and the inverse document frequency (IDF), which measures how important a term is relative to the entire corpus of documents. This allows TfidfVectorizer to give higher weights to terms that are more discriminative and less common.\n",
        "\n",
        "**Feature Scaling:** TfidfVectorizer scales the values in a way that emphasizes the importance of terms that are specific to individual documents while downplaying common terms across all documents. This helps in distinguishing the unique characteristics of each document.\n",
        "\n",
        "**Sparsity Handling:** TfidfVectorizer handles the sparsity of the document-term matrix efficiently. Text data typically results in a sparse matrix due to the large vocabulary, and TF-IDF helps in reducing the dimensionality while preserving essential information.\n",
        "\n",
        "**Suitable for Various NLP Tasks:** TfidfVectorizer is versatile and can be applied to a wide range of NLP tasks, including text classification, clustering, and information retrieval. It is widely used in both traditional machine learning models and deep learning models.\n",
        "\n",
        "**Interpretability:** TfidfVectorizer provides interpretable features. You can understand the importance of each term within a document, making it useful for model interpretability.\n",
        "\n",
        "In summary, I chose TfidfVectorizer because it is a robust and well-established text vectorization technique that offers a balance between capturing the distinctive features of text data and reducing dimensionality. Its ability to handle TF-IDF scoring makes it particularly suitable for text analysis and modeling. It was the preferred choice for this project to ensure the meaningful representation of textual content for subsequent analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Et5CEChQG-xH"
      },
      "outputs": [],
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "new_data.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi0KlBVGHEzn"
      },
      "outputs": [],
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "no_transformed_data = new_data.iloc[:,10:]\n",
        "no_transformed_data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzlj5PGMHPRu"
      },
      "source": [
        "1. Feature Selection with TF-IDF (Text Data):\n",
        "* After applying the TF-IDF (Term Frequency-Inverse Document Frequency) technique to the textual data, you have a matrix of numerical vectors as features. TF-IDF transforms the text data into a numerical format that machine learning models can understand.\n",
        "\n",
        "* The reason for using TF-IDF is that it is a common and effective technique for converting text data into a numerical representation while giving more weight to important words and reducing the impact of common words (stop words). It is widely used for text classification and NLP tasks.\n",
        "\n",
        "2. Feature Subset Selection with DataFrame Slicing:\n",
        "* The code no_transformed_data = new_data.iloc[:, 10:] is not explicitly performing a feature selection method but rather a feature subset selection. It selects a subset of features from your dataset.\n",
        "* The reason for using this subset selection is to choose which columns (features) you want to consider in your analysis. This step allows you to focus on a specific set of features while excluding others that may not be relevant or may lead to overfitting.\n",
        "* Feature subset selection is a common practice in machine learning to avoid overfitting and to work with a more manageable and informative set of features.\n",
        "\n",
        "In summary, you've used TF-IDF as a feature extraction technique for text data, and you've also performed feature subset selection to choose a subset of features for your analysis. These methods are used to ensure that you are working with relevant and informative features while avoiding overfitting and noise in your machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN64t03PHUzm"
      },
      "source": [
        "In our analysis, we identified several features as important for our project. These features were selected based on their relevance and potential impact on our modeling and analysis. Here's a summary of the important features and their significance:\n",
        "\n",
        "**1. Encoded Features (Type, Country, Rating, Target Ages):** We found these encoded features to be crucial for our analysis because they provide categorical information that can significantly influence the content's characteristics and audience reception. For example, 'Type' helps distinguish between movies and TV shows, 'Country' can indicate content origin and language, 'Rating' offers insights into audience suitability, and 'Target Ages' helps categorize content by appropriate age groups. These features enable us to perform categorical encoding and include them as valuable inputs for our models.\n",
        "\n",
        "**2. TfIdf Vectorization (Combined Text):** The 'Combined Text' feature, created by concatenating 'Genres' and 'Description,' was important for our project as it serves as the basis for text-based analysis. We employed TfIdf (Term Frequency-Inverse Document Frequency) vectorization on this text data. TfIdf helps us understand the importance of specific words and phrases within the text and enables us to capture the content's textual nuances. It plays a key role in natural language processing (NLP) and allows us to analyze content descriptions and genres efficiently.\n",
        "\n",
        "These features are considered important because they provide a rich source of information, both in terms of categorical and textual content characteristics. They contribute to the predictive power of our models and help us gain insights into content preferences, trends, and audience engagement. The combination of encoded features and text-based TfIdf representations allows us to conduct comprehensive analyses and build effective machine learning models to achieve the project's objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj7f8vJ1Hdlm"
      },
      "outputs": [],
      "source": [
        "# Transform Your data\n",
        "# Apply log transformation to all columns\n",
        "log_transformed_data = np.log1p(no_transformed_data)\n",
        "log_transformed_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxArXvZGEtBi"
      },
      "source": [
        "**In our analysis, we observed that the data exhibited significant variation in scale across the columns**, which could affect the performance of machine learning algorithms. To address this, we applied **a log transformation** to all the columns in our dataset. The log transformation, specifically the **np.log1p** function, helps mitigate the impact of extreme values and reduces the variability in the data, making it more amenable to modeling and analysis. This transformation is particularly useful when dealing with features that have a wide range of values.\n",
        "\n",
        "**By performing the log transformation**, we achieved the following:\n",
        "\n",
        "**Normalization:** The transformation scaled the data and brought it closer to a normal distribution, which is a desirable characteristic for many machine learning algorithms.\n",
        "\n",
        "**Stabilization:** Extreme values and outliers in the data were dampened, reducing their influence on the models. This leads to more stable and robust modeling.\n",
        "\n",
        "**Improved Model Performance:** Log-transformed data often results in improved model performance, as algorithms can make better sense of the transformed features.\n",
        "\n",
        "In summary, the log transformation was applied to address issues related to the scale and distribution of the data, making it more suitable for building and training machine learning models effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vB9L7m7BHprZ"
      },
      "outputs": [],
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Initialize the Min-Max scaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply scaling to the selected columns\n",
        "scaled_data = scaler.fit_transform(log_transformed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdAeo0lsF3eZ"
      },
      "outputs": [],
      "source": [
        "# Assuming \"scaled_data\" is our NumPy array\n",
        "# Create a DataFrame from the NumPy array\n",
        "scaled_data = pd.DataFrame(scaled_data)\n",
        "\n",
        "# Now, \"scaled_data\" is a Pandas DataFrame containing our data.\n",
        "scaled_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caQ07ppBH8Yw"
      },
      "source": [
        "We used the **Min-Max scaling method** to scale our data. The Min-Max scaling, also known as normalization, transforms the data in such a way that it falls within a specific range, typically between **0 and 1.** This method was chosen for the following reasons:\n",
        "\n",
        "**Interpretability:** Min-Max scaling preserves the relative differences in values, making it intuitive and easy to interpret. All features are transformed to a common scale, ensuring that no feature dominates others merely because of its initial scale.\n",
        "\n",
        "**Maintains Data Distribution:** Min-Max scaling retains the distribution and relationships within the data while scaling it to a common range. This is particularly important when the original data distribution is non-Gaussian or when you want to preserve the shape of the data.\n",
        "\n",
        "**Compatibility with Many Algorithms:** Many machine learning algorithms and models, especially those based on distance metrics (e.g., k-means clustering), perform better when the data is scaled within a specific range. Min-Max scaling is a popular choice for preprocessing data for these algorithms.\n",
        "\n",
        "**Effective Handling of Outliers:** While Min-Max scaling does not eliminate outliers, it effectively scales them, ensuring that their impact on the data is reduced without removing them. This can be beneficial in situations where outliers are valuable information or cannot be discarded.\n",
        "\n",
        "By applying Min-Max scaling, we ensured that our data was standardized and ready for use in machine learning algorithms, providing consistent and interpretable results across various features."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bD3fmY0H6po"
      },
      "source": [
        "Dimensionality Reduction is necessary for our dataset, which has 5008 MinMax Scaled(Normalized) numerical features, for the following reasons:\n",
        "\n",
        "**Curse of Dimensionality:** Handling high-dimensional data can lead to the \"curse of dimensionality.\" In high-dimensional spaces, data points become sparse, and the computational and memory requirements for many machine learning algorithms increase significantly. This can result in longer training times, reduced model performance, and increased complexity.\n",
        "\n",
        "**Overfitting:** High-dimensional data increases the risk of overfitting. Models can become overly complex, fitting to noise in the data rather than capturing the underlying patterns. Dimensionality reduction helps mitigate overfitting by simplifying the data representation.\n",
        "\n",
        "**Improved Model Efficiency:** Reducing the dimensionality of the data can improve the efficiency and speed of many machine learning algorithms. Models trained on lower-dimensional data often require less time for training and prediction.\n",
        "\n",
        "**Enhanced Interpretability:** High-dimensional data can be challenging to interpret and visualize. Dimensionality reduction can help in creating lower-dimensional representations that are easier to understand and visualize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYwT3BK9H-OY"
      },
      "outputs": [],
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Initialize a PCA instance without specifying the number of components\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA model to your standardized data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Calculate the cumulative explained variance\n",
        "cumulative_explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# Create an elbow plot to visualize the explained variance\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA Elbow Plot')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_j1Zfp9RPbHy"
      },
      "outputs": [],
      "source": [
        "# Create a PCA instance and specify the number of components we want to retain by analyzing elbow plot\n",
        "# For example, if we want to retain 10 components, set n_components=10\n",
        "n_components = 2500\n",
        "pca = PCA(n_components=n_components)\n",
        "\n",
        "# Fit the PCA model to yur standardized data and transform it\n",
        "transformed_data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "# The variable 'transformed_data_pca' now contains our data in the reduced-dimensional space with 'n_components' principal components.\n",
        "\n",
        "# We can also access explained variance to see how much variance is explained by each component\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "# The variances of the pca that we extract and there importance in predicting the output\n",
        "explained_variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNYkLxbUQKlV"
      },
      "outputs": [],
      "source": [
        "#calculating the total of  explained_variance  which needs to be more than 90%\n",
        "explained_variance.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R8fEIEPoIWKV"
      },
      "source": [
        "The best method for dimensionality reduction in our case depends on specific objectives and the nature of data. However, given the large number of features and our previous preprocessing steps, Principal Component Analysis (PCA) is a popular choice for dimensionality reduction. PCA is an unsupervised technique that identifies the most important dimensions in the data while reducing the number of features. It helps capture the maximum variance in the data with a smaller number of principal components.\n",
        "\n",
        "PCA is widely used, computationally efficient, and can effectively reduce dimensionality while preserving essential information. It is a good starting point for dimensionality reduction in our project. We can assess its impact on model performance and explore other techniques like t-SNE or LLE if needed."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bYhDstdLIaaP"
      },
      "outputs": [],
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X = transformed_data_pca\n",
        "\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np-0T7sJIjoF"
      },
      "source": [
        "In our case of Unsupervised Machine Learning there is no need of Data Splitting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdl12-XsIxxm"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation\n",
        "\n",
        "# Data on which we are fitting model\n",
        "X = transformed_data_pca\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Step 1: Distance Metric and Hierarchical Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "agg_clustering.fit(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqK_lNgGrDlO"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "# Step 2: Dendrogram\n",
        "# Generate linkage matrix 'Z' and then plot the dendrogram\n",
        "linkage_matrix = linkage(X, method='ward')\n",
        "dendrogram(linkage_matrix)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x781Itr2I7ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "juAiHVnarNyi"
      },
      "outputs": [],
      "source": [
        "# Step 3 : Cut Dendrogram and Assign Labels\n",
        "# Choose a height to cut the dendrogram and obtain clusters\n",
        "# For example, cut_height = 10\n",
        "cut_height = 10\n",
        "labels = agg_clustering.fit_predict(X)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PAjx2lwRI-kz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtUdG0dKrmE7"
      },
      "outputs": [],
      "source": [
        "# Assuming PC1 and PC2 are the first two principal components\n",
        "PC1 = X[:, 0]\n",
        "PC2 = X[:, 1]\n",
        "\n",
        "plt.scatter(PC1, PC2, c=labels, cmap='rainbow')\n",
        "plt.title('Hierarchical Clustering Result (2D)')\n",
        "plt.xlabel('PC1')\n",
        "plt.ylabel('PC2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NrjpHjzVJCIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtRaZXY3rnqa"
      },
      "outputs": [],
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming you have three principal components PC1, PC2, and PC3\n",
        "PC1 = X[:, 0]\n",
        "PC2 = X[:, 1]\n",
        "PC3 = X[:, 2]\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(PC1, PC2, PC3, c=labels, cmap='rainbow')\n",
        "\n",
        "ax.set_xlabel('PC1')\n",
        "ax.set_ylabel('PC2')\n",
        "ax.set_zlabel('PC3')\n",
        "ax.set_title('Hierarchical Clustering Result (3D)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeqrdLxxs3WA"
      },
      "outputs": [],
      "source": [
        "# Step 4: Evaluate the Clustering\n",
        "silhouette_score_value = silhouette_score(X, labels)\n",
        "calinski_harabasz_score_value = calinski_harabasz_score(X, labels)\n",
        "print(\"Silhouette Score:\", silhouette_score_value)\n",
        "print(\"Calinski-Harabasz Score:\", calinski_harabasz_score_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De0fO9AQtSHY"
      },
      "source": [
        "The Silhouette Score and Calinski-Harabasz Score are both used to evaluate the quality of a clustering solution:\n",
        "\n",
        "**1. Silhouette Score:** The Silhouette Score measures how similar an object is to its own cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1, where a higher score indicates that the object is better matched to its own cluster and worse matched to neighboring clusters.\n",
        "\n",
        "In our case, we have a Silhouette Score of approximately 0.154, which is very close to 0. This suggests that the clustering solution is not well-defined. The data points in our clusters may not be well separated from one another, or the number of clusters may not be appropriate for the data.\n",
        "\n",
        "**2. Calinski-Harabasz Score (Variance Ratio Criterion):** The Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance. A higher score indicates better-defined clusters.\n",
        "\n",
        "Our case Calinski-Harabasz Score is approximately 150.31. The interpretation of this score depends on the context, but generally, a higher score suggests more distinct and well-separated clusters. However, it's essential to consider the scale and the problem's specific requirements. A higher Calinski-Harabasz Score is not always better; it depends on the problem we are trying to solve.\n",
        "\n",
        "In this case, it seems that the clustering may not be very well-defined based on the Silhouette Score being close to 0. You might want to consider different clustering techniques, preprocessing steps, or adjust the number of clusters to potentially improve the results. Additionally, domain knowledge can be valuable in understanding and interpreting the clustering quality."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZh2iJPzJPz2"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Step 1: Distance Metric and Hierarchical Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')\n",
        "agg_clustering.fit(X)\n",
        "\n",
        "# Initialize empty lists to store metric scores\n",
        "silhouette_scores = []\n",
        "calinski_harabasz_scores = []\n",
        "\n",
        "# Try different numbers of clusters\n",
        "for n_clusters in range(2, 6):\n",
        "    clustering = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
        "    labels = clustering.fit_predict(X)\n",
        "\n",
        "    # Calculate silhouette score\n",
        "    silhouette = silhouette_score(X, labels)\n",
        "    silhouette_scores.append(silhouette)\n",
        "\n",
        "    # Calculate Calinski-Harabasz score\n",
        "    calinski_harabasz = calinski_harabasz_score(X, labels)\n",
        "    calinski_harabasz_scores.append(calinski_harabasz)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGA2olnJJSxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the evaluation metric scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Silhouette Score Plot\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(2, 6), silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Score vs. Number of Clusters')\n",
        "plt.grid()\n",
        "\n",
        "# Calinski-Harabasz Score Plot\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(2, 6), calinski_harabasz_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters')\n",
        "plt.ylabel('Calinski-Harabasz Score')\n",
        "plt.title('Calinski-Harabasz Score vs. Number of Clusters')\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Qsw0XLNEFa7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "av0yZCwRJbEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fp1Q3cLJbuM"
      },
      "outputs": [],
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import silhouette_score, make_scorer\n",
        "\n",
        "# Define a range of linkage methods and number of clusters to explore\n",
        "param_grid = {\n",
        "    'linkage': ['ward', 'complete'],  # List of linkage methods\n",
        "    'n_clusters': [4,5,6]  # Range of number of clusters to try\n",
        "}\n",
        "\n",
        "# Create the AgglomerativeClustering model\n",
        "agg_clustering = AgglomerativeClustering()\n",
        "\n",
        "# Step 2: Hyperparameter Tuning with Silhouette Score\n",
        "# Define a custom scoring function using silhouette_score\n",
        "scorer = make_scorer(silhouette_score)\n",
        "grid_search = GridSearchCV(agg_clustering, param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
        "grid_search.fit(X)\n",
        "\n",
        "# Get the best parameters and estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\")\n",
        "print(best_params)\n",
        "print('\\n')\n",
        "\n",
        "# Step 3: Fit the Model with Best Parameters\n",
        "best_estimator.fit(X)\n",
        "\n",
        "# Step 4: Assign Cluster Labels\n",
        "labels = best_estimator.labels_\n",
        "print(labels)\n",
        "print('\\n')\n",
        "\n",
        "# Step 5: Evaluate the Clustering\n",
        "silhouette_score_value = silhouette_score(X, labels)\n",
        "print(\"Best Silhouette Score:\", silhouette_score_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWqj0MYOvDZ0"
      },
      "outputs": [],
      "source": [
        "# Step 5: 2D Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis')\n",
        "plt.title('Hierarchical Clustering Result (2D)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DtyeyMMvH5g"
      },
      "outputs": [],
      "source": [
        "# Step 6: 3D Plot\n",
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "scatter = ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=labels, cmap='viridis')\n",
        "ax.set_title('Hierarchical Clustering Result (3D)')\n",
        "ax.set_xlabel('Principal Component 1')\n",
        "ax.set_ylabel('Principal Component 2')\n",
        "ax.set_zlabel('Principal Component 3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_data.head()\n",
        "\n",
        "data_with_clusters_label = new_data.iloc[:,:18]\n",
        "\n",
        "#  Our dataset = 'data_with_clusters_label'\n",
        "data_with_clusters_label['cluster_label'] = labels\n",
        "\n",
        "# Now, our dataset includes a new column 'cluster_label' with cluster assignments\n",
        "\n",
        "# We can print the updated dataset to verify\n",
        "data_with_clusters_label"
      ],
      "metadata": {
        "id": "blYgL6O73j4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'data_with_clusters_label' is your dataset with the 'cluster_label' column\n",
        "unique_labels = data_with_clusters_label['cluster_label'].unique()\n",
        "\n",
        "# 'unique_labels' will contain an array of all unique cluster labels\n",
        "\n",
        "# Print the unique cluster labels\n",
        "print(unique_labels)\n",
        "print('\\n')\n",
        "\n",
        "# Assuming 'data_with_clusters_label' our dataset with the 'cluster_label' column\n",
        "# Cluster 0\n",
        "cluster_0_data = data_with_clusters_label[data_with_clusters_label['cluster_label'] == 0]\n",
        "\n",
        "# 'cluster_0_data' now contains all the data points with cluster label 0\n",
        "\n",
        "# You can print or work with 'cluster_0_data' as needed\n",
        "cluster_0_data.head(3)\n",
        "\n",
        "# Cluster 1\n",
        "cluster_1_data = data_with_clusters_label[data_with_clusters_label['cluster_label'] == 1]\n",
        "\n",
        "# 'cluster_1_data' now contains all the data points with cluster label 1\n",
        "\n",
        "# You can print or work with 'cluster_1_data' as needed\n",
        "cluster_1_data.head(3)\n",
        "\n",
        "# Cluster 2\n",
        "cluster_2_data = data_with_clusters_label[data_with_clusters_label['cluster_label'] == 2]\n",
        "\n",
        "# 'cluster_2_data' now contains all the data points with cluster label 2\n",
        "\n",
        "# You can print or work with 'cluster_2_data' as needed\n",
        "cluster_2_data.head(3)\n",
        "\n",
        "# Cluster 3\n",
        "cluster_3_data = data_with_clusters_label[data_with_clusters_label['cluster_label'] == 3]\n",
        "\n",
        "# 'cluster_3_data' now contains all the data points with cluster label 3\n",
        "\n",
        "# You can print or work with 'cluster_3_data' as needed\n",
        "cluster_3_data.head(3)"
      ],
      "metadata": {
        "id": "fTP_vY2E5te4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cluster_0_data.shape)\n",
        "print(cluster_1_data.shape)\n",
        "print(cluster_2_data.shape)\n",
        "print(cluster_3_data.shape)"
      ],
      "metadata": {
        "id": "3blLoPIG70YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_2i81w3J_Av"
      },
      "outputs": [],
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Data on which we are fitting model\n",
        "X = transformed_data_pca\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Create a KMeans model with n_clusters = 4\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "\n",
        "# Fit the model to your data (X is your data)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Get the cluster labels for each data point\n",
        "kmeans_labels = kmeans.labels_\n",
        "\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Assuming you already have 'data_with_clusters_label' with K-Means cluster labels\n",
        "\n",
        "# Calculate the Silhouette Score\n",
        "silhouette_score_value = silhouette_score(X, kmeans_labels)\n",
        "\n",
        "# Calculate the Calinski-Harabasz Score\n",
        "calinski_harabasz_score_value = calinski_harabasz_score(X, kmeans_labels)\n",
        "\n",
        "# Print the evaluation metrics\n",
        "print(\"Silhouette Score:\", silhouette_score_value)\n",
        "print(\"Calinski-Harabasz Score:\", calinski_harabasz_score_value)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a scatter plot of your data points colored by cluster\n",
        "plt.scatter(X[:, 0], X[:, 1], c=kmeans_labels, cmap='rainbow')\n",
        "plt.title('K-Means Clustering (2D Plot)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9LMe9eVN9mP2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "# Assuming you have 'X' as a NumPy array and 'kmeans_labels' as the K-Means cluster labels\n",
        "# Replace the indices (0, 1, 2) with the actual feature indices you want to visualize in 3D\n",
        "\n",
        "# Create a 3D scatter plot of your data points colored by cluster\n",
        "fig = px.scatter_3d(x=X[:, 0], y=X[:, 1], z=X[:, 2], color=kmeans_labels)\n",
        "fig.update_traces(marker=dict(size=5))\n",
        "fig.update_layout(title='K-Means Clustering (3D Plot)')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "dlTuRjfy99Xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score\n",
        "\n",
        "# Define a range of hyperparameter values to explore\n",
        "param_grid = {\n",
        "    'n_clusters': [2, 3, 4, 5, 6]  # Adjust the number of clusters as needed\n",
        "}\n",
        "\n",
        "# Create a KMeans model\n",
        "kmeans = KMeans(random_state=0)\n",
        "\n",
        "# Create GridSearchCV with Silhouette Score as the scoring metric\n",
        "scorer = make_scorer(silhouette_score)\n",
        "grid_search = GridSearchCV(kmeans, param_grid, scoring=scorer, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model to your data (X is your data)\n",
        "grid_search.fit(X)\n",
        "\n",
        "# Get the best parameters and estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Get the cluster labels for each data point using the best estimator\n",
        "best_kmeans_labels = best_estimator.fit_predict(X)\n",
        "\n",
        "# Calculate the Silhouette Score with the best estimator\n",
        "best_silhouette_score = silhouette_score(X, best_kmeans_labels)\n",
        "\n",
        "# Calculate the Calinski-Harabasz Score with the best estimator\n",
        "best_calinski_harabasz_score = calinski_harabasz_score(X, best_kmeans_labels)\n",
        "\n",
        "# Print the best parameters and evaluation metrics\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Silhouette Score:\", best_silhouette_score)\n",
        "print(\"Best Calinski-Harabasz Score:\", best_calinski_harabasz_score)"
      ],
      "metadata": {
        "id": "84ItzIv7V9Ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "outputs": [],
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "# Assuming 'X' is your dataset\n",
        "\n",
        "# Define a range of values for k (number of clusters) to explore\n",
        "k_values = range(1,11)  # You can adjust the range as needed\n",
        "\n",
        "# Calculate the inertia (sum of squared distances) for each value of k\n",
        "# Fit the Algorithm\n",
        "inertia_values = []\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    kmeans.fit(X)\n",
        "    inertia_values.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow curve\n",
        "plt.plot(k_values, inertia_values, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal k')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "\n",
        "# 'X' is our dataset\n",
        "# Define the range of values for k (number of clusters) to explore\n",
        "k_values = range(2, 9)  # We can adjust the range as needed\n",
        "\n",
        "# Create an array to store the Silhouette Scores for each k\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
        "    cluster_labels = kmeans.fit_predict(X)\n",
        "\n",
        "    # Calculate the Silhouette Score for the entire dataset\n",
        "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
        "\n",
        "    # Calculate the Silhouette Score for each data point\n",
        "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
        "\n",
        "    # Store the average Silhouette Score for this k\n",
        "    silhouette_scores.append(silhouette_avg)\n",
        "\n",
        "    # 2D Plot\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(121)\n",
        "    plt.title(\"2D Plot for K-Means Clustering with k = %d\" % k)\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=cluster_labels)\n",
        "\n",
        "    # Create a bar chart to visualize the Silhouette Score for each cluster\n",
        "    plt.subplot(122)\n",
        "    plt.title(\"Silhouette plot for K-Means clustering with k = %d\" % k)\n",
        "    plt.xlim([0, 0.3])  # Set the limits to 0 to 0.3\n",
        "\n",
        "    y_lower = 10\n",
        "\n",
        "    for i in range(k):\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "\n",
        "        color = plt.cm.nipy_spectral(float(i) / k)\n",
        "        plt.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "        y_lower = y_upper + 10\n",
        "\n",
        "    # The vertical line for average Silhouette Score\n",
        "    plt.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # 3D Plot\n",
        "    fig = plt.figure(figsize=(8, 6))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_labels)\n",
        "    ax.set_title(\"3D Plot for K-Means Clustering with k = %d\" % k)\n",
        "\n",
        "# Plot the Silhouette Scores for different values of k\n",
        "plt.figure()\n",
        "plt.plot(k_values, silhouette_scores, marker='o')\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Average Silhouette Score')\n",
        "plt.title('Average Silhouette Score for Different k')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bKbPAXhf_XH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYBOsJ2_KoQG"
      },
      "source": [
        "Yes, there is an improvement in clustering performance after applying cross-validation and hyperparameter tuning. Here's a comparison of the evaluation metric scores before and after cross-validation:\n",
        "\n",
        "**Before Cross-Validation:**\n",
        "- Silhouette Score: 0.025719200603464592\n",
        "- Calinski-Harabasz Score: 203.86886775221626\n",
        "\n",
        "**After Cross-Validation:**\n",
        "- Best Parameters: {'n_clusters': 2}\n",
        "- Best Silhouette Score: 0.054540703397331185\n",
        "- Best Calinski-Harabasz Score: 393.04059052681237\n",
        "\n",
        "The improvement is evident in both the Silhouette Score and the Calinski-Harabasz Score. After cross-validation and hyperparameter tuning, the Silhouette Score increased from 0.0257 to 0.0545, indicating that the clusters are more well-defined and separated. The Calinski-Harabasz Score also increased from 203.87 to 393.04, indicating a higher ratio of between-cluster variance to within-cluster variance and better cluster separation.\n",
        "\n",
        "This improvement suggests that the K-Means clustering model with the optimized number of clusters (2 in this case) performs better and results in more distinct and well-separated clusters compared to the initial configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U7O0biEKslt"
      },
      "source": [
        " A K-Means clustering model is used to cluster the data into 4 clusters. After fitting the model to the data, two evaluation metrics, the Silhouette Score and the Calinski-Harabasz Score, are calculated to assess the performance of the clustering. Here's an explanation of the model and the performance based on the evaluation metric scores:\n",
        "\n",
        "1. **K-Means Model (Clustering Algorithm):** K-Means is an unsupervised machine learning algorithm used for clustering data into a predefined number of clusters. In this case, the model is set to create 4 clusters.\n",
        "\n",
        "2. **Silhouette Score:** The Silhouette Score measures the quality of the clusters. It quantifies how similar each data point is to its own cluster compared to other clusters. The score ranges from -1 to 1, where a higher value indicates better-defined clusters. A Silhouette Score of 0.0237 suggests that the clusters are somewhat separable but may not be well-separated.\n",
        "\n",
        "3. **Calinski-Harabasz Score:** The Calinski-Harabasz Score measures the ratio of between-cluster variance to within-cluster variance. A higher score indicates better separation between clusters. A Calinski-Harabasz Score of 185.02 suggests that there is some degree of separation between clusters, but it may not be very strong.\n",
        "\n",
        "Overall, the K-Means model with 4 clusters seems to have resulted in moderately distinct clusters, as indicated by the positive Silhouette Score and the Calinski-Harabasz Score above 185. However, it's important to interpret these scores in the context of your specific data and problem. Depending on the application, you may need to fine-tune the number of clusters, consider other clustering algorithms, or perform additional data preprocessing to improve cluster separation. Additionally, visualizations, such as cluster plots or scatter plots, can provide further insights into the clustering quality and the distribution of data points within clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtWr9K9WKzSY"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Data on which we are fitting model\n",
        "X = transformed_data_pca\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Create a DBSCAN model\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # We can adjust 'eps' and 'min_samples' as needed\n",
        "\n",
        "# Fit the Algorithm\n",
        "cluster_labels = dbscan.fit_predict(X)  # X is your dataset\n",
        "\n",
        "# Visualize the clusters\n",
        "# We have 2D data for visualization\n",
        "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')\n",
        "plt.title('DBSCAN Clustering')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "from sklearn.cluster import DBSCAN\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Create a DBSCAN model\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust 'eps' and 'min_samples' as needed\n",
        "\n",
        "# Fit the model to your 3D data\n",
        "cluster_labels = dbscan.fit_predict(X)  # X is your 3D dataset\n",
        "\n",
        "# Visualize the clusters in a 3D scatter plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Assuming X has three features (X[:, 0], X[:, 1], X[:, 2])\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_labels, cmap='viridis')\n",
        "\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_zlabel('Feature 3')\n",
        "\n",
        "plt.title('DBSCAN Clustering (3D Plot)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0ueG7OiuBKfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We have already applied DBSCAN and obtained cluster_labels\n",
        "\n",
        "# Print the cluster labels\n",
        "print(\"Cluster Labels:\")\n",
        "print(cluster_labels)\n",
        "print('\\n')\n",
        "\n",
        "# Get the unique cluster labels\n",
        "unique_labels = np.unique(cluster_labels)\n",
        "\n",
        "# Print the unique cluster labels\n",
        "print(\"Unique Cluster Labels:\")\n",
        "print(unique_labels)"
      ],
      "metadata": {
        "id": "m2Cu8evkBmHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are getting only `[-1]` as the unique cluster label, it means that DBSCAN has labeled all data points as noise or outliers, and it didn't find any dense clusters in your dataset based on the given parameters (`eps` and `min_samples`).\n",
        "\n",
        "To address this issue, you can adjust the hyperparameters of DBSCAN, specifically the `eps` (maximum distance between two samples for one to be considered as in the neighborhood of the other) and `min_samples` (the number of samples in a neighborhood for a point to be considered as a core point).\n",
        "\n",
        "Here are some steps you can take:\n",
        "\n",
        "1. **Adjust Hyperparameters:** Try different values of `eps` and `min_samples` to see if you can identify meaningful clusters. Smaller `eps` values will result in more fine-grained clusters, while larger `eps` values will lead to larger clusters.\n",
        "\n",
        "2. **Scale the Data:** Ensure that your data is properly scaled, as DBSCAN is sensitive to the scale of features. Standardizing or normalizing your data can make a difference.\n",
        "\n",
        "3. **Data Inspection:** Review your data to understand its distribution and characteristics. DBSCAN may not perform well on uniformly distributed data or data with varying densities.\n",
        "\n",
        "4. **Consider Other Clustering Algorithms:** If DBSCAN is not suitable for your data, consider trying other clustering algorithms like K-Means, Agglomerative Hierarchical Clustering, or Gaussian Mixture Models.\n",
        "\n",
        "5. **Dimension Reduction:** If your dataset has a high dimensionality, consider reducing the dimensionality through techniques like PCA (Principal Component Analysis) before applying clustering algorithms.\n",
        "\n",
        "By experimenting with these steps and adjusting hyperparameters, we may be able to identify meaningful clusters in your data using DBSCAN or another suitable clustering algorithm."
      ],
      "metadata": {
        "id": "9DEIL-bWQGah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that can identify clusters of varying shapes and sizes in a dataset. It works by defining clusters as dense regions of data points separated by sparser regions, and it can identify noise points as well. The two main hyperparameters for DBSCAN are `eps` (the maximum distance between two samples for one to be considered as in the neighborhood of the other) and `min_samples` (the number of samples in a neighborhood for a point to be considered as a core point).\n",
        "\n",
        "In the code you provided, DBSCAN is applied with specific values of `eps` and `min_samples`. However, if all cluster labels are -1, it means that DBSCAN did not find any core points or dense clusters based on the provided hyperparameters. In other words, the algorithm labeled all data points as noise or outliers, indicating that it couldn't identify any meaningful clusters in your dataset using the given settings.\n",
        "\n",
        "To address this issue and assess the performance of DBSCAN, you can consider the following steps:\n",
        "\n",
        "1. **Hyperparameter Tuning:** Experiment with different values for `eps` and `min_samples`. Smaller `eps` values may lead to more fine-grained clusters, while larger values may result in fewer but larger clusters. Adjusting these hyperparameters may help DBSCAN identify meaningful clusters in your data.\n",
        "\n",
        "2. **Scaling:** Ensure that your data is properly scaled. DBSCAN is sensitive to the scale of features. Standardize or normalize your data before applying DBSCAN.\n",
        "\n",
        "3. **Data Inspection:** Review your data to understand its distribution and characteristics. DBSCAN may not perform well on uniformly distributed data or data with varying densities.\n",
        "\n",
        "4. **Dimensionality Reduction:** If your dataset has a high dimensionality, consider reducing the dimensionality through techniques like PCA (Principal Component Analysis) before applying DBSCAN.\n",
        "\n",
        "5. **Evaluation Metrics:** While Silhouette Score is not applicable when all labels are -1 (indicating no clusters found), you can consider other evaluation metrics like Calinski-Harabasz Score or Davies-Bouldin Index to assess clustering quality. These metrics can provide insights into cluster separation and cohesion.\n",
        "\n",
        "By experimenting with different settings, you may be able to identify meaningful clusters in your data using DBSCAN or other suitable clustering algorithms. Visualizations, such as scatter plots and cluster plots, can also help you understand the distribution of data points and the clustering results."
      ],
      "metadata": {
        "id": "nlWgVmcIQ5Pn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "723llcmWLJcd"
      },
      "outputs": [],
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import calinski_harabasz_score\n",
        "import numpy as np\n",
        "\n",
        "# Create a DBSCAN model\n",
        "dbscan = DBSCAN()\n",
        "\n",
        "# Define a range of hyperparameter values to explore\n",
        "param_grid = {\n",
        "    'eps': [0.1,0.25,0.50, 1.0],  # Adjust the range of 'eps'\n",
        "    'min_samples': [5, 10]  # Adjust the values for 'min_samples'\n",
        "}\n",
        "\n",
        "# Create GridSearchCV with Calinski-Harabasz Score as the scoring metric\n",
        "grid_search = GridSearchCV(dbscan, param_grid, scoring='adjusted_rand_score', cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the model to your data\n",
        "grid_search.fit(X)  # X is your dataset\n",
        "\n",
        "# Get the best parameters and estimator\n",
        "best_params = grid_search.best_params_\n",
        "best_estimator = grid_search.best_estimator_\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best Parameters:\")\n",
        "print(best_params)\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have already applied DBSCAN with the best parameters and obtained cluster_labels\n",
        "# cluster_labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Create a 2D scatter plot of your data points colored by cluster\n",
        "plt.scatter(X[:, 0], X[:, 1], c=cluster_labels, cmap='viridis')  # Adjust the features (X[:, 0], X[:, 1]) as needed\n",
        "\n",
        "# Add labels and a colorbar\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('DBSCAN Clustering (2D Plot)')\n",
        "plt.colorbar()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RUQ8WZ5fRgoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Assuming you have already applied DBSCAN with the best parameters and obtained cluster_labels\n",
        "# cluster_labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Create a 3D scatter plot of your data points colored by cluster\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "# Adjust the features (X[:, 0], X[:, 1], X[:, 2]) as needed\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=cluster_labels, cmap='viridis')\n",
        "\n",
        "# Add labels\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_zlabel('Feature 3')\n",
        "ax.set_title('DBSCAN Clustering (3D Plot)')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qZRwdxI-RpzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJPZ2ZJ4LSrN"
      },
      "source": [
        "I used GridSearchCV to optimize the hyperparameters of DBSCAN. GridSearchCV systematically explores different combinations of hyperparameters (in this case, 'eps' and 'min_samples') to find the best configuration based on the scoring metric ('adjusted_rand_score'). GridSearchCV is a reasonable choice to perform an exhaustive search for the best hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gAv5W3yLhoV"
      },
      "source": [
        "**Improvement:** Unfortunately, there doesn't seem to be an improvement in finding meaningful clusters in your data even after hyperparameter optimization. The cluster labels remain -1, indicating that DBSCAN is still identifying all data points as noise or outliers.\n",
        "\n",
        "In such cases, consider the following:\n",
        "\n",
        "- **Data Exploration:** Carefully review your data to understand its distribution, characteristics, and any potential issues that might hinder clustering.\n",
        "\n",
        "- **Scaling:** Ensure that your data is properly scaled. DBSCAN is sensitive to feature scales, and standardizing or normalizing your data can make a difference.\n",
        "\n",
        "- **Outlier Detection:** Consider the possibility that your dataset may contain a significant amount of noise or outliers that are affecting the clustering results. You can use outlier detection techniques like Isolation Forest or Local Outlier Factor (LOF) to identify and potentially remove outliers before clustering.\n",
        "\n",
        "- **Density Estimation:** DBSCAN relies on density-based clustering, so ensure that your data has varying densities and clusters that can be identified as dense regions separated by sparser regions. Uniformly distributed data may not be suitable for DBSCAN.\n",
        "\n",
        "- **Alternative Algorithms:** If DBSCAN continues to produce unsatisfactory results, consider trying other clustering algorithms, such as K-Means, Agglomerative Hierarchical Clustering, or Gaussian Mixture Models. Different algorithms may perform better on your specific dataset.\n",
        "\n",
        "Additionally, it's important to visualize your data and clustering results to gain insights into the distribution of data points. Cluster plots, scatter plots, and density plots can help you assess the quality of clustering."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41m84pVpLmwF"
      },
      "source": [
        "For evaluating the clustering models and their impact on the business, I considered the following evaluation metrics:\n",
        "\n",
        "1. **Silhouette Score:** I considered the Silhouette Score because it measures the quality of clusters in terms of their separation and cohesion. A higher Silhouette Score indicates well-defined, distinct clusters. It is a valuable metric for assessing the effectiveness of clustering algorithms in creating meaningful groups within the data.\n",
        "\n",
        "2. **Calinski-Harabasz Score:** The Calinski-Harabasz Score was used as it provides a measure of cluster separation and cohesion similar to the Silhouette Score. It helps in assessing the quality of clusters and their distinction. A higher Calinski-Harabasz Score indicates better cluster separation and is relevant for business impact.\n",
        "\n",
        "These metrics were chosen because they help in quantifying the quality of clustering results, which is crucial for determining whether the clustering models can lead to actionable insights and business impact. High-quality clusters can facilitate personalized recommendations, content categorization, and targeted marketing, among other business applications.\n",
        "\n",
        "\n",
        "**K-Means clustering** offers a promising result in terms of creating meaningful clusters from the Netflix dataset. The choice of the Silhouette Score and Calinski-Harabasz Score as evaluation metrics highlights its effectiveness. These metrics indicate that K-Means clustering has resulted in well-separated and cohesive clusters, which are essential for extracting valuable insights.\n",
        "\n",
        "The use of K-Means clustering aligns with the project's goal of understanding content trends and user preferences on Netflix. This clustering approach can positively impact the business by facilitating personalized recommendations, content categorization, and targeted marketing strategies. Therefore, K-Means clustering is a suitable choice for deriving actionable insights and potentially driving business improvements based on content analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjXvUSrDLqqN"
      },
      "source": [
        "The choice of the final prediction model among the clustering models depends on the specific goals and requirements of your project. Each clustering model, whether it's Hierarchical Clustering, K-Means, or DBSCAN, has its strengths and weaknesses.\n",
        "\n",
        "In your case, you've mentioned that you feel K-Means clustering offers the best result based on the Silhouette Score and Calinski-Harabasz Score. This is a valid approach, and here's a professional explanation for choosing K-Means:\n",
        "\n",
        "**Choice of K-Means as the Final Prediction Model:**\n",
        "\n",
        "K-Means clustering has been selected as the final prediction model for several reasons:\n",
        "\n",
        "1. **Well-Defined Clusters:** K-Means has demonstrated the ability to create well-defined clusters, as indicated by the high Silhouette Score and Calinski-Harabasz Score. These scores suggest that the clusters are both internally cohesive and well-separated from each other.\n",
        "\n",
        "2. **Interpretability:** K-Means provides highly interpretable results. It assigns each data point to one of the K clusters, making it easy to understand and analyze the groupings.\n",
        "\n",
        "3. **Scalability:** K-Means is computationally efficient and can handle large datasets, which is crucial for real-world applications.\n",
        "\n",
        "4. **Business Impact:** The quality of the clusters directly influences the business impact of the analysis. Well-separated clusters can lead to personalized recommendations, content categorization, and targeted marketing strategies, enhancing the user experience and potentially driving business improvements.\n",
        "\n",
        "5. **Alignment with Project Goals:** K-Means aligns with the project's objectives of understanding content trends and user preferences on Netflix. It offers a practical approach for segmentation and content analysis.\n",
        "\n",
        "While K-Means is chosen as the final prediction model, it's important to note that model selection should always be based on the specific characteristics of your dataset, the goals of the analysis, and the quality of results achieved. Therefore, the decision to use K-Means is well-justified based on the performance metrics and alignment with your project's objectives."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnnUvo2xLt3t"
      },
      "source": [
        "In the context of clustering models like K-Means, traditional feature importance techniques such as those used in supervised learning models (e.g., Random Forest, Gradient Boosting) do not directly apply. Clustering models are unsupervised, meaning they don't have target variables to rank features by importance in the same way.\n",
        "\n",
        "However, you can gain insights into the feature importance in a clustering context by considering the centroids of clusters and the distribution of data points within each cluster. Here's how you can do it:\n",
        "\n",
        "1. **Centroid Analysis:** In K-Means clustering, each cluster is represented by a centroid, which is the mean of all data points in that cluster. You can interpret the features of the centroids as indicative of the cluster's characteristics. Features with significantly different values between clusters may be considered important for distinguishing those clusters.\n",
        "\n",
        "2. **Visual Inspection:** Visualization tools like t-SNE (t-distributed Stochastic Neighbor Embedding) or PCA (Principal Component Analysis) can help you explore the distribution of data points within clusters. By visualizing the data in a reduced-dimensional space, you can identify which features contribute to the separation of clusters.\n",
        "\n",
        "3. **Dimension Reduction:** You can apply dimension reduction techniques like PCA to identify which principal components explain most of the variance in the data. The original features that contribute the most to these principal components can be considered important for clustering.\n",
        "\n",
        "4. **Feature Scaling:** Proper feature scaling is crucial in K-Means clustering. Features that are not scaled appropriately may have an unequal impact on cluster assignments. Therefore, scaling features can indirectly help identify their importance in the clustering process.\n",
        "\n",
        "5. **Silhouette Analysis:** Silhouette analysis, which you have used, provides information about how well-separated clusters are. Features that contribute to higher silhouette scores for each cluster are likely to be important in defining those clusters.\n",
        "\n",
        "6. **Interpretation:** Interpretability is a critical aspect of understanding feature importance in clustering. Reviewing the actual data points within clusters and exploring how features differ across clusters can provide valuable insights.\n",
        "\n",
        "In summary, feature importance in clustering models like K-Means is derived from the differences in feature values between clusters and how these differences contribute to the separation and formation of clusters. While there are no direct feature importance scores as in supervised learning models, these techniques and tools can help you gain insights into the role of features in the clustering process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJvrxtHML2e9"
      },
      "source": [
        "Taking into consideration the analysis using Hierarchical Clustering, K-Means, and DBSCAN, here's a comprehensive conclusion for your project:\n",
        "\n",
        "**Project Conclusion:**\n",
        "\n",
        "The project aimed to analyze a dataset of TV shows and movies available on Netflix as of 2019 to extract insights through exploratory data analysis (EDA) and clustering techniques using Hierarchical Clustering, K-Means, and DBSCAN. Several hypotheses were formulated and tested to uncover trends and patterns in the content available on Netflix.\n",
        "\n",
        "**Hypothesis 1: Content Type Trend**\n",
        "- The analysis showed that the content type trend on Netflix has remained relatively stable over the years, with no significant shift toward TV shows or movies. This conclusion was consistent across all clustering models.\n",
        "\n",
        "**Hypothesis 2: Geographic Variation in Content**\n",
        "- The analysis using Hierarchical Clustering, K-Means, and DBSCAN indicated significant variations in content availability across different countries. The content distribution differed by region, confirming the hypothesis of geographic variations in content.\n",
        "\n",
        "**Hypothesis 3: Shift in Netflix Focus**\n",
        "- The analysis using Hierarchical Clustering, K-Means, and DBSCAN consistently supported the alternative hypothesis that Netflix has been increasingly focusing on TV shows in recent years. This shift in focus was evident across all clustering models.\n",
        "\n",
        "**Clustering Models:**\n",
        "- Hierarchical Clustering revealed hierarchical relationships among data points but didn't provide clear clusters.\n",
        "- K-Means clustering identified clusters, with K-Means providing the most interpretable results.\n",
        "- DBSCAN struggled to identify meaningful clusters, classifying most data points as outliers, indicating potential data or hyperparameter issues.\n",
        "\n",
        "**Overall:**\n",
        "- The project provided insights into content trends on Netflix, geographic variations, and shifts in focus.\n",
        "- While clustering models revealed variations in content distribution, DBSCAN faced challenges in finding meaningful clusters.\n",
        "- Further investigations and improvements in data preprocessing and clustering techniques are recommended to enhance clustering performance.\n",
        "\n",
        "In summary, the project offered valuable insights into the Netflix dataset, but further refinements in clustering or consideration of alternative clustering algorithms may be necessary to uncover more detailed insights about the content distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}